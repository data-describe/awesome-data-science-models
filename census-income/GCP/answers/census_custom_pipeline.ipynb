{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f929bdca-53dd-4ef4-be08-6d69f383607d",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1be29069-5b96-4ac0-b796-96f93c81fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9da84703-fdac-4f60-b200-add7c587c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install --upgrade --user google-cloud-aiplatform \\\n",
    "#                                google-cloud-storage \\\n",
    "#                                kfp \\\n",
    "#                                google-cloud-pipeline-components -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27922218-3b32-4235-81f0-652bc49547c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://aaa-aca-ml-workshop-central'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJECT_ID = \"ds-training-380514\"\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_NAME = \"aaa-aca-ml-workshop-central\"\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "\n",
    "BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19499602-6213-4b69-8e35-b0294d82b7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get this value from GCP IAM console\n",
    "SERVICE_ACCOUNT = \"354621994428-compute@developer.gserviceaccount.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b61a866-9069-4dc3-827e-b8f9056d2d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform, bigquery\n",
    "from kfp.dsl import pipeline\n",
    "from kfp.v2 import compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44558fc7-7a60-4dbf-9b2e-c37d120c89a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI SDK\n",
    "aiplatform.init(project=PROJECT_ID,\n",
    "                location=REGION,\n",
    "                staging_bucket=BUCKET_URI)\n",
    "\n",
    "# Initialize BigQuery client\n",
    "bq_client = bigquery.Client(\n",
    "                            project=PROJECT_ID,\n",
    "                            credentials=aiplatform.initializer.global_config.credentials,\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7eab5532-3533-48c1-9d29-8bf0493200d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "# Generate a uuid of a specifed length(default=8)\n",
    "def generate_uuid(length: int = 8) -> str:\n",
    "    return \"\".join(random.choices(string.ascii_lowercase + string.digits,\n",
    "                                  k=length))\n",
    "\n",
    "UUID = generate_uuid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1781f7e-410a-4f96-ab50-5576cb4b3b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source of the dataset\n",
    "DATA_SOURCE = \"bq://bigquery-public-data.ml_datasets.census_adult_income\"\n",
    "\n",
    "# Set name for the managed Vertex AI dataset\n",
    "DATASET_DISPLAY_NAME = f\"adult_census_dataset_{UUID}\"\n",
    "\n",
    "# BigQuery Dataset name\n",
    "BQ_DATASET_ID = f\"income_prediction_{UUID}\"\n",
    "\n",
    "# Set name for the BigQuery source table for batch prediction\n",
    "BQ_INPUT_TABLE = f\"income_test_data_{UUID}\"\n",
    "\n",
    "# Set the size(%) of the train set\n",
    "TRAIN_SPLIT = 0.9\n",
    "\n",
    "# Provide the container for training the model\n",
    "TRAINING_CONTAINER = \"us-docker.pkg.dev/vertex-ai/training/scikit-learn-cpu.0-23:latest\"\n",
    "# Provide the container for serving the model\n",
    "SERVING_CONTAINER = \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-23:latest\"\n",
    "\n",
    "# Set the display name for training job\n",
    "TRAINING_JOB_DISPLAY_NAME = f\"income_classify_train_job_{UUID}\"\n",
    "\n",
    "# Model display name for Vertex AI Model Registry\n",
    "MODEL_DISPLAY_NAME = f\"income_classify_model_{UUID}\"\n",
    "# Set the name for batch prediction job\n",
    "BATCH_PREDICTION_JOB_NAME = f\"income_classify_batch_pred_{UUID}\"\n",
    "# Dispaly name for the Vertex AI Pipeline\n",
    "PIPELINE_DISPLAY_NAME = f\"income_classfiy_batch_pred_pipeline_{UUID}\"\n",
    "\n",
    "# Filename to compile the pipeline to\n",
    "PIPELINE_FILE_NAME = f\"{PIPELINE_DISPLAY_NAME}.json\"  # to be generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33ed199b-bd1e-4ce9-8b3b-7f005bd13e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset ds-training-380514.income_prediction_qqoshgp7\n"
     ]
    }
   ],
   "source": [
    "# Create a BQ dataset\n",
    "bq_dataset = bigquery.Dataset(f\"{PROJECT_ID}.{BQ_DATASET_ID}\")\n",
    "bq_dataset = bq_client.create_dataset(bq_dataset)\n",
    "print(f\"Created dataset {bq_client.project}.{bq_dataset.dataset_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ccdd7c95-c408-4f1e-83aa-c60a508b46b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to create a test set from the source table\n",
    "query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE\n",
    "  `{PROJECT_ID}.{BQ_DATASET_ID}.{BQ_INPUT_TABLE}` AS\n",
    "\n",
    "SELECT\n",
    "  * EXCEPT (pseudo_random, income_bracket)\n",
    "FROM (\n",
    "  SELECT\n",
    "    *,\n",
    "    RAND() AS pseudo_random \n",
    "  FROM\n",
    "    `bigquery-public-data.ml_datasets.census_adult_income` )\n",
    "WHERE pseudo_random > {TRAIN_SPLIT}\n",
    "\"\"\"\n",
    "# Run the query\n",
    "_ = bq_client.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061c3f74-9160-49b0-84e0-d7b49068ab63",
   "metadata": {},
   "source": [
    "## Prepare custom model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3206bb39-2844-486e-8beb-61c466a83912",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p python_package\n",
    "!mkdir -p python_package/trainer\n",
    "!touch python_package/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6960f8fa-aceb-40fe-b7af-a031ab4629fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting python_package/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile python_package/trainer/task.py\n",
    "import os\n",
    "import joblib\n",
    "import argparse\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Read environmental variables\n",
    "PROJECT = os.getenv(\"CLOUD_ML_PROJECT_ID\")\n",
    "TRAINING_DATA_URI = os.getenv(\"AIP_TRAINING_DATA_URI\")\n",
    "\n",
    "# Set Bigquery Client\n",
    "bq_client = bigquery.Client(project=PROJECT)\n",
    "storage_client = storage.Client(project=PROJECT)\n",
    "\n",
    "# Define the constants\n",
    "TARGET = 'income_bracket'\n",
    "ARTIFACTS_PATH = os.getenv(\"AIP_MODEL_DIR\")\n",
    "# Get the bucket name from the model dir\n",
    "BUCKET_NAME = ARTIFACTS_PATH.replace(\"gs://\",\"\").split(\"/\")[0]\n",
    "\n",
    "MODEL_FILENAME = 'model.joblib' \n",
    "# Define the format of your input data, excluding the target column.\n",
    "COLUMNS = [\n",
    "           'age',\n",
    "           'workclass',\n",
    "           'functional_weight',\n",
    "           'education',\n",
    "           'education_num',\n",
    "           'marital_status',\n",
    "           'occupation',\n",
    "           'relationship',\n",
    "           'race',\n",
    "           'sex',\n",
    "           'capital_gain',\n",
    "           'capital_loss',\n",
    "           'hours_per_week',\n",
    "           'native_country'\n",
    "           ]\n",
    "\n",
    "CATEGORICAL_COLUMNS = [\n",
    "                       'workclass',\n",
    "                       'education',\n",
    "                       'marital_status',\n",
    "                       'occupation',\n",
    "                       'relationship',\n",
    "                       'race',\n",
    "                       'sex',\n",
    "                       'native_country'\n",
    "                      ]\n",
    "\n",
    "# Fetch data from BigQuery\n",
    "def download_table(bq_table_uri: str):\n",
    "    prefix = \"bq://\"\n",
    "    if bq_table_uri.startswith(prefix):\n",
    "        bq_table_uri = bq_table_uri[len(prefix):]\n",
    "\n",
    "    table = bigquery.TableReference.from_string(bq_table_uri)\n",
    "    rows = bq_client.list_rows(table,)\n",
    "    return rows.to_dataframe(create_bqstorage_client=False)\n",
    "\n",
    "# Upload local files to GCS\n",
    "def upload_model(bucket_name: str,\n",
    "                filename: str):\n",
    "     # Upload the saved model file to GCS\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    storage_path = os.path.join(ARTIFACTS_PATH, filename)\n",
    "    blob = storage.blob.Blob.from_string(storage_path,\n",
    "                                         client=storage_client)\n",
    "    blob.upload_from_filename(filename)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load the training data\n",
    "    X_train = download_table(TRAINING_DATA_URI)\n",
    "\n",
    "    # Remove the column we are trying to predict ('income-level') from our features list\n",
    "    # Convert the Dataframe to a lists of lists\n",
    "    train_features = X_train.drop(TARGET, axis=1).to_numpy().tolist()\n",
    "    # Create our training labels list, convert the Dataframe to a lists of lists\n",
    "    train_labels = X_train[TARGET].to_numpy().tolist()\n",
    "\n",
    "    # We use a list of pipelines to convert each categorical column and then use FeatureUnion to combine them\n",
    "    # before calling the RandomForestClassifier.\n",
    "    categorical_pipelines = []\n",
    "\n",
    "    # Each categorical column needs to be extracted individually and converted to a numerical value.\n",
    "    # To do this, each categorical column uses a pipeline that extracts one feature column via\n",
    "    # SelectKBest(k=1) and a LabelBinarizer() to convert the categorical value to a numerical one.\n",
    "    # A scores array (created below) selects and extracts the feature column. The scores array is\n",
    "    # created by iterating over the COLUMNS and checking if it is a CATEGORICAL_COLUMN.\n",
    "    for i, col in enumerate(COLUMNS):\n",
    "        if col in CATEGORICAL_COLUMNS:\n",
    "            # Create a scores array to get the individual categorical column.\n",
    "            # Example:\n",
    "            #  data = [39, 'State-gov', 77516, 'Bachelors', 13, 'Never-married', 'Adm-clerical',\n",
    "            #         'Not-in-family', 'White', 'Male', 2174, 0, 40, 'United-States']\n",
    "            #  scores = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "            #\n",
    "            # Returns: [['Sate-gov']]\n",
    "            \n",
    "            scores = []\n",
    "            # Build the scores array\n",
    "            for j in range(len(COLUMNS)):\n",
    "                if i == j: # This column is the categorical column we want to extract.\n",
    "                    scores.append(1) # Set to 1 to select this column\n",
    "                else: # Every other column should be ignored.\n",
    "                    scores.append(0)\n",
    "                    \n",
    "            skb = SelectKBest(k=1)\n",
    "            skb.scores_ = scores\n",
    "            # Convert the categorical column to a numerical value\n",
    "            lbn = LabelBinarizer()\n",
    "            r = skb.transform(train_features)\n",
    "            lbn.fit(r)\n",
    "            # Create the pipeline to extract the categorical feature\n",
    "            categorical_pipelines.append(\n",
    "                                        ('categorical-{}'.format(i), Pipeline([\n",
    "                                                                                ('SKB-{}'.format(i), skb),\n",
    "                                                                                ('LBN-{}'.format(i), lbn)\n",
    "                                                                              ])\n",
    "                                        )\n",
    "                                        )\n",
    "\n",
    "    # Create pipeline to extract the numerical features\n",
    "    skb = SelectKBest(k=6)\n",
    "    # From COLUMNS use the features that are numerical\n",
    "    skb.scores_ = [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0]\n",
    "    categorical_pipelines.append(('numerical', skb))\n",
    "\n",
    "    # Combine all the features using FeatureUnion\n",
    "    preprocess = FeatureUnion(categorical_pipelines)\n",
    "\n",
    "    # Create the classifier\n",
    "    classifier = RandomForestClassifier()\n",
    "\n",
    "    # Transform the features and fit them to the classifier\n",
    "    classifier.fit(preprocess.transform(train_features), train_labels)\n",
    "\n",
    "    # Create the overall model as a single pipeline\n",
    "    pipeline = Pipeline([\n",
    "                         ('union', preprocess),\n",
    "                         ('classifier', classifier)\n",
    "                        ])\n",
    "\n",
    "    # Save the pipeline locally\n",
    "    joblib.dump(pipeline, MODEL_FILENAME)\n",
    "    \n",
    "    # Upload the locally saved model to GCS\n",
    "    upload_model(\n",
    "                 bucket_name = BUCKET_NAME, \n",
    "                 filename=MODEL_FILENAME\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c011f8-26f4-4513-b7ed-24a69e623a93",
   "metadata": {},
   "source": [
    "## Packaging of custom model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "283621ef-630b-4e97-9f88-d4a6a2452388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting python_package/setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile python_package/setup.py\n",
    "\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = ['pandas','pyarrow']\n",
    "\n",
    "setup(\n",
    "      name='trainer',\n",
    "      version='0.1',\n",
    "      packages=find_packages(),\n",
    "      include_package_data=True,\n",
    "      description='My training application.'\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2cad4a6-b4cb-4c08-90ff-6b3910c4af51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing trainer.egg-info/PKG-INFO\n",
      "writing dependency_links to trainer.egg-info/dependency_links.txt\n",
      "writing top-level names to trainer.egg-info/top_level.txt\n",
      "reading manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'trainer.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "running check\n",
      "creating trainer-0.1\n",
      "creating trainer-0.1/trainer\n",
      "creating trainer-0.1/trainer.egg-info\n",
      "copying files to trainer-0.1...\n",
      "copying setup.py -> trainer-0.1\n",
      "copying trainer/__init__.py -> trainer-0.1/trainer\n",
      "copying trainer/task.py -> trainer-0.1/trainer\n",
      "copying trainer.egg-info/PKG-INFO -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/SOURCES.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/dependency_links.txt -> trainer-0.1/trainer.egg-info\n",
      "copying trainer.egg-info/top_level.txt -> trainer-0.1/trainer.egg-info\n",
      "Writing trainer-0.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'trainer-0.1' (and everything under it)\n"
     ]
    }
   ],
   "source": [
    "!cd python_package && python3 setup.py sdist --formats=gztar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc343c1f-488d-4b08-8169-0726ee4acd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://python_package/dist/trainer-0.1.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  2.7 KiB/  2.7 KiB]                                                \n",
      "Operation completed over 1 objects/2.7 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp -r python_package/dist/* $BUCKET_URI/training_package/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f4d4c9-3530-479a-b6da-a6b367f1a236",
   "metadata": {},
   "source": [
    "## Build pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6729138-d685-412c-a6de-e7e10daea621",
   "metadata": {},
   "source": [
    "Using pre-built components\n",
    "\n",
    "Reference: https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-1.0.0/google_cloud_pipeline_components.aiplatform.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c6cf83a-dc08-4011-b4b1-fdefb925f4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(name=\"custom-model-bq-batch-prediction-pipeline\")\n",
    "def custom_model_bq_batch_prediction_pipeline(\n",
    "                                            project: str,\n",
    "                                            location: str,\n",
    "                                            dataset_display_name: str,\n",
    "                                            dataset_bq_source: str,\n",
    "                                            training_job_dispaly_name: str,\n",
    "                                            gcs_staging_directory: str,\n",
    "                                            python_package_gcs_uri: str,\n",
    "                                            python_package_module_name: str,\n",
    "                                            training_split: float,\n",
    "                                            test_split: float,\n",
    "                                            training_container_uri: str,\n",
    "                                            serving_container_uri: str,\n",
    "                                            training_bigquery_destination: str,\n",
    "                                            model_display_name: str,\n",
    "                                            batch_prediction_display_name: str,\n",
    "                                            batch_prediction_instances_format: str,\n",
    "                                            batch_prediction_predictions_format: str,\n",
    "                                            batch_prediction_source_uri: str,\n",
    "                                            batch_prediction_destination_uri: str,\n",
    "                                            batch_prediction_machine_type: str = \"n1-standard-4\",\n",
    "                                            batch_prediction_batch_size: int = 1000,\n",
    "                                             ):\n",
    "    \n",
    "    from google_cloud_pipeline_components.aiplatform import (\n",
    "        CustomPythonPackageTrainingJobRunOp, ModelBatchPredictOp,\n",
    "        TabularDatasetCreateOp)\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset_create_op = TabularDatasetCreateOp(\n",
    "                                                project=project,\n",
    "                                                location=location,\n",
    "                                                display_name=dataset_display_name,\n",
    "                                                bq_source=dataset_bq_source,\n",
    "                                              )\n",
    "\n",
    "    # Run the training task\n",
    "    train_op = CustomPythonPackageTrainingJobRunOp(\n",
    "                                                    display_name=training_job_dispaly_name,\n",
    "                                                    python_package_gcs_uri=python_package_gcs_uri,\n",
    "                                                    python_module_name=python_package_module_name,\n",
    "                                                    container_uri=training_container_uri,\n",
    "                                                    model_display_name=model_display_name,\n",
    "                                                    model_serving_container_image_uri=serving_container_uri,\n",
    "                                                    dataset=dataset_create_op.outputs[\"dataset\"],\n",
    "                                                    base_output_dir=gcs_staging_directory,\n",
    "                                                    bigquery_destination=training_bigquery_destination,\n",
    "                                                    training_fraction_split=training_split,\n",
    "                                                    test_fraction_split=test_split,\n",
    "                                                    staging_bucket=gcs_staging_directory,\n",
    "                                                  )\n",
    "\n",
    "    # Run the batch prediction task\n",
    "    _ = ModelBatchPredictOp(\n",
    "                            project=project,\n",
    "                            location=location,\n",
    "                            model=train_op.outputs[\"model\"],\n",
    "                            instances_format=batch_prediction_instances_format,\n",
    "                            bigquery_source_input_uri=batch_prediction_source_uri,\n",
    "                            predictions_format=batch_prediction_predictions_format,\n",
    "                            bigquery_destination_output_uri=batch_prediction_destination_uri,\n",
    "                            job_display_name=batch_prediction_display_name,\n",
    "                            machine_type=batch_prediction_machine_type,\n",
    "                            manual_batch_tuning_parameters_batch_size=batch_prediction_batch_size,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4baf564-d1bc-4edb-a0f9-4b854d7c9b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "                            pipeline_func=custom_model_bq_batch_prediction_pipeline,\n",
    "                            package_path=PIPELINE_FILE_NAME,\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "46f11487-f421-4120-ab9b-b9ac20409bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "            \"project\": PROJECT_ID,\n",
    "            \"location\": REGION,\n",
    "            \"dataset_display_name\": DATASET_DISPLAY_NAME,\n",
    "            \"dataset_bq_source\": DATA_SOURCE,\n",
    "            \"training_job_dispaly_name\": TRAINING_JOB_DISPLAY_NAME,\n",
    "            \"gcs_staging_directory\": BUCKET_URI,\n",
    "            \"python_package_gcs_uri\": f\"{BUCKET_URI}/training_package/trainer-0.1.tar.gz\",\n",
    "            \"python_package_module_name\": \"trainer.task\",\n",
    "            \"training_split\": TRAIN_SPLIT,\n",
    "            \"test_split\": 1 - TRAIN_SPLIT,\n",
    "            \"training_container_uri\": TRAINING_CONTAINER,\n",
    "            \"serving_container_uri\": SERVING_CONTAINER,\n",
    "            \"training_bigquery_destination\": f\"bq://{PROJECT_ID}\",\n",
    "            \"model_display_name\": MODEL_DISPLAY_NAME,\n",
    "            \"batch_prediction_display_name\": BATCH_PREDICTION_JOB_NAME,\n",
    "            \"batch_prediction_instances_format\": \"bigquery\",\n",
    "            \"batch_prediction_predictions_format\": \"bigquery\",\n",
    "            \"batch_prediction_source_uri\": f\"bq://{PROJECT_ID}.{BQ_DATASET_ID}.{BQ_INPUT_TABLE}\",\n",
    "            \"batch_prediction_destination_uri\": f\"bq://{PROJECT_ID}.{BQ_DATASET_ID}\",\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b280044f-4855-4153-a0d4-5358f6dffdd2",
   "metadata": {},
   "source": [
    "## Run pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da6f9378-01bd-4d33-9454-eae478c142be",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "                            display_name=PIPELINE_DISPLAY_NAME,\n",
    "                            template_path=PIPELINE_FILE_NAME,\n",
    "                            parameter_values=parameters,\n",
    "                            enable_caching=True,\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "427a8993-9e26-4d66-a3b6-ba3cda237962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/354621994428/locations/us-central1/pipelineJobs/custom-model-bq-batch-prediction-pipeline-20230406192358\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/354621994428/locations/us-central1/pipelineJobs/custom-model-bq-batch-prediction-pipeline-20230406192358')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/custom-model-bq-batch-prediction-pipeline-20230406192358?project=354621994428\n",
      "PipelineJob projects/354621994428/locations/us-central1/pipelineJobs/custom-model-bq-batch-prediction-pipeline-20230406192358 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/354621994428/locations/us-central1/pipelineJobs/custom-model-bq-batch-prediction-pipeline-20230406192358 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/354621994428/locations/us-central1/pipelineJobs/custom-model-bq-batch-prediction-pipeline-20230406192358 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/354621994428/locations/us-central1/pipelineJobs/custom-model-bq-batch-prediction-pipeline-20230406192358 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/354621994428/locations/us-central1/pipelineJobs/custom-model-bq-batch-prediction-pipeline-20230406192358 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/354621994428/locations/us-central1/pipelineJobs/custom-model-bq-batch-prediction-pipeline-20230406192358 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/354621994428/locations/us-central1/pipelineJobs/custom-model-bq-batch-prediction-pipeline-20230406192358 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/354621994428/locations/us-central1/pipelineJobs/custom-model-bq-batch-prediction-pipeline-20230406192358 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/354621994428/locations/us-central1/pipelineJobs/custom-model-bq-batch-prediction-pipeline-20230406192358 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/354621994428/locations/us-central1/pipelineJobs/custom-model-bq-batch-prediction-pipeline-20230406192358\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline job\n",
    "job.run(service_account=SERVICE_ACCOUNT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
