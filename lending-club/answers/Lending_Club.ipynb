{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HP Tuning on Vertex AI\n",
    "This notebook uses the Lending Club dataset to create an XGBoost model and run hyperparameter tuning in Vertex AI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train your model on Vertex AI with HP tuning.\n",
    "Using HP Tuning for training can be done in a few steps:\n",
    "1. Create your python model file\n",
    "    1. Add argument parsing for the hyperparameter values. (These values are chosen for you in this notebook)\n",
    "    1. Add code to download your data from [Google Cloud Storage](https://cloud.google.com/storage) so that AI Platform can use it\n",
    "    1. Add code to track the performance of your hyperparameter values.\n",
    "    1. Add code to export and save the model to [Google Cloud Storage](https://cloud.google.com/storage) once AI Platform finishes training the model\n",
    "1. Prepare a package\n",
    "1. Submit the training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "Before you jump in, let’s cover some of the different tools you’ll be using to get HP tuning up and running on AI Platform. \n",
    "\n",
    "[Google Cloud Platform](https://cloud.google.com/) lets you build and host applications and websites, store data, and analyze data on Google's scalable infrastructure.\n",
    "\n",
    "[Vertex AI](https://cloud.google.com/vertex-ai/docs/training/custom-training) provides a custom training service that enables you to easily build machine learning models that work on any type of data, of any size.\n",
    "\n",
    "[Google Cloud Storage](https://cloud.google.com/storage/) (GCS) is a unified object storage for developers and enterprises, from live data serving to data analytics/ML to data archiving.\n",
    "\n",
    "[Cloud SDK](https://cloud.google.com/sdk/) is a command line tool which allows you to interact with Google Cloud products. In order to run this notebook, make sure that Cloud SDK is [installed](https://cloud.google.com/sdk/downloads) in the same environment as your Jupyter kernel.\n",
    "\n",
    "[Overview of Hyperparameter Tuning](https://cloud.google.com/ml-engine/docs/tensorflow/hyperparameter-tuning-overview) - Hyperparameter tuning takes advantage of the processing infrastructure of Google Cloud Platform to test different hyperparameter configurations when training your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Setup\n",
    "* [Create a project on GCP](https://cloud.google.com/resource-manager/docs/creating-managing-projects)\n",
    "* [Create a Google Cloud Storage Bucket](https://cloud.google.com/storage/docs/quickstart-console)\n",
    "* [Enable AI Platform Training and Prediction and Compute Engine APIs](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,compute_component&_ga=2.217405014.1312742076.1516128282-1417583630.1516128282)\n",
    "* [Install Cloud SDK](https://cloud.google.com/sdk/downloads)\n",
    "* [Install XGBoost](https://xgboost.readthedocs.io/en/latest/build.html) [Optional: used if running locally]\n",
    "* [Install pandas](https://pandas.pydata.org/pandas-docs/stable/install.html) [Optional: used if running locally]\n",
    "* [Install cloudml-hypertune](https://pypi.org/project/cloudml-hypertune/) [Optional: used if running locally]\n",
    "\n",
    "In the cell below, **replace** the following highlighted elements:\n",
    "* `project <PROJECT_ID>` - with this project id (i.e. ai-platform-demo)\n",
    "* `bucket <BUCKET_ID>` - with your student id (i.e. maven-student01)\n",
    "* `folder <FOLDER>` - with something about this exercise (i.e. lending_club)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace <PROJECT_ID>, <BUCKET_ID>, and <FOLDER> with proper Project, Bucket ID, and Folder.\n",
    "project = 'mwpmltr'\n",
    "bucket = 'mwpmltr-vertex'\n",
    "folder = 'vertex'\n",
    "region = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROJECT_ID=mwpmltr\n",
      "env: BUCKET_ID=mwpmltr-vertex\n",
      "env: BUCKET_PATH=mwpmltr-vertex/vertex\n",
      "Creating gs://mwpmltr-vertex/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'mwpmltr-vertex' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "bucket_path=f'{bucket}/{folder}'\n",
    "%env PROJECT_ID=$project\n",
    "%env BUCKET_ID=$bucket\n",
    "%env BUCKET_PATH=$bucket_path\n",
    "!gsutil mb -c standard -l {region} gs://{bucket}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These variables will be needed for the following steps.\n",
    "* `REGION <us-central1>` - select a region from [here](https://cloud.google.com/ml-engine/docs/regions) or use the default 'us-central1'. The region is where the model will be deployed.\n",
    "* `TRAINER_PACKAGE_PATH <./trainer>` - A packaged training application that will be staged in a Google Cloud Storage location. The model file created below is placed inside this package path.\n",
    "* `MAIN_TRAINER_MODULE <trainer.task>` - Tells Vertex AI which file to execute. This is formatted as follows <folder_name.python_file_name>\n",
    "* `HPTUNING_CONFIG <hptuning_config.yaml>` - Path to the job configuration file.\n",
    "* `EXECUTOR_IMAGE_URL <us-docker.pkg.dev/vertex-ai/training/xgboost-cpu.1-1:latest>` - The link to a pre-built XGBoost container image for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: REGION=us-central1\n",
      "env: TRAINER_PACKAGE_PATH=.\n",
      "env: MAIN_TRAINER_MODULE=trainer.task\n",
      "env: HPTUNING_CONFIG=hptuning_config.yaml\n",
      "env: EXECUTOR_IMAGE_URI=us-docker.pkg.dev/vertex-ai/training/xgboost-cpu.1-1:latest\n"
     ]
    }
   ],
   "source": [
    "%env REGION us-central1\n",
    "%env TRAINER_PACKAGE_PATH .\n",
    "%env MAIN_TRAINER_MODULE trainer.task\n",
    "%env HPTUNING_CONFIG hptuning_config.yaml\n",
    "%env EXECUTOR_IMAGE_URI us-docker.pkg.dev/vertex-ai/training/xgboost-cpu.1-1:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "\n",
    " * File is `lending_club_data.tsv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "This dataset is provided by a third party. Google provides no representation,\n",
    "warranty, or other guarantees about the validity or any other aspects of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://amazing-public-data/lending_club/lending_club_data.tsv [Content-Type=text/tab-separated-values]...\n",
      "/ [1 files][  4.2 MiB/  4.2 MiB]                                                \n",
      "Operation completed over 1 objects/4.2 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "# Download the data\n",
    "!gsutil cp gs://amazing-public-data/lending_club/lending_club_data.tsv gs://${BUCKET_PATH}/lending_club_data.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Create your Python model file\n",
    "\n",
    "We have created the Python model file (inside trainer folder) that we'll upload to Vertex AI. This is similar to your normal process for creating an XGBoost model. However, there are a few key differences:\n",
    "1. Downloading the data from GCS at the start of your file, so that Vertex AI can access the data.\n",
    "1. Exporting/saving the model to GCS at the end of your file, so that you can use it for predictions.\n",
    "1. Define a command-line argument in your main training module for Vertex AI parameters and for each tuned hyperparameter.\n",
    "1. Use the values passed in those arguments to set the corresponding Vertex AI parameters and hyperparameters in your application's XGBoost code.\n",
    "1. Use `cloudml-hypertune` to track your training jobs metrics.\n",
    "\n",
    "The code in this file first handles the parameters and hyperparameters passed to the file from Vertex AI. Then it loads the data into a pandas DataFrame that can be used by XGBoost. Then the model is fit against the training data and the metrics for that data are shared with Vertex AI. Lastly, Python's built in pickle library is used to save the model to a file that can be uploaded to [Vertex AI's prediction service](https://cloud.google.com/ml-engine/docs/scikit/getting-predictions#deploy_models_and_versions).\n",
    "\n",
    "Note: In normal practice you would want to test your model locally on a small dataset to ensure that it works, before using it with your larger dataset on Vertex AI. This avoids wasted time and costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Create Trainer Package with Hyperparameter Tuning\n",
    "The Trainer Package holds all your code and dependencies need to train your model on Vertex AI. \n",
    "\n",
    "The trainer folder and its contents can be referred to as the Trainer Package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to set the hp tuning values used to train our model. Check [HyperparameterSpec](https://cloud.google.com/ml-engine/reference/rest/v1/projects.jobs#HyperparameterSpec) for more info. \n",
    "\n",
    "In this config file several key things are set:\n",
    "* `maxTrials` - How many training trials should be attempted to optimize the specified hyperparameters.\n",
    "* `maxParallelTrials: 5` - The number of training trials to run concurrently. \n",
    "* `params` - The set of parameters to tune. These are the different parameters to pass into your model and the specified ranges you wish to try.\n",
    " * `parameterName` - The parameter name must be unique amongst all ParameterConfigs\n",
    " * `type` - The type of the parameter. [INTEGER, DOUBLE, ...]\n",
    " * `minValue` & `maxValue` - The range of values that this parameter could be. \n",
    " * `scaleType` - How the parameter should be scaled to the hypercube. Leave unset for categorical parameters. Some kind of scaling is strongly recommended for real or integral parameters (e.g., UNIT_LINEAR_SCALE).\n",
    " \n",
    " \n",
    " \n",
    "`hptuning_config.yaml` is the config file that we will be using here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need to install the dependencies used in our model. Check [adding_standard_pypi_dependencies](https://cloud.google.com/ml-engine/docs/tensorflow/packaging-trainer#adding_standard_pypi_dependencies) for more info.\n",
    "\n",
    "To do this, Vertex AI uses a setup.py file to install your dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Submit Training Job\n",
    "Next we need to submit the job for training on Vertex AI. We'll use gcloud to submit the job which has the following flags:\n",
    "\n",
    "* `display-name` - A name to use for the job (mixed-case letters, numbers, and underscores only, starting with a letter). In this case: `auto_mpg_hp_tuning_$(date +\"%Y%m%d_%H%M%S\")`\n",
    "* `config` - Path to the job configuration file. This file should be a YAML document (JSON also accepted) containing a Job resource as defined in the API\n",
    "* `region` - The Google Cloud Compute region where you want your job to run. You should run your training job in the same region as the Cloud Storage bucket that stores your training data. Select a region from [here](https://cloud.google.com/ml-engine/docs/regions) or use the default '`us-central1`'.\n",
    "* `worker-pool-spec` - Define the worker pool configuration used by the custom job\n",
    "    * `executor-image-uri` - A link to a [pre-built container](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers) to use for training.\n",
    "    * `local-package-path` - The local path of a folder that contains training code.\n",
    "    * `python-module` - The name of the main module in your trainer package. The main module is the Python file you call to start the application. Refer to Python Packages to figure out the module name.\n",
    "    * `machine-type` - The type of the machine.\n",
    "* `args` - A comma separated list of custom parameters used in the Python file\n",
    "\n",
    "Note: Check to make sure gcloud is set to the current PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workaround for Docker in Vertex AI Notebook\n",
    "!sudo chmod 666 /var/run/docker.sock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOB_NAME=lending_club_job_20210724_231846\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Sending build context to Docker daemon  68.66kB\n",
      "Step 1/10 : FROM us-docker.pkg.dev/vertex-ai/training/xgboost-cpu.1-1:latest\n",
      " ---> 6da3b9be283f\n",
      "Step 2/10 : RUN mkdir -m 777 -p /usr/app /home\n",
      " ---> Running in a12f0419fb58\n",
      "Removing intermediate container a12f0419fb58\n",
      " ---> 466e5ffe6846\n",
      "Step 3/10 : WORKDIR /usr/app\n",
      " ---> Running in 069e4134d8e7\n",
      "Removing intermediate container 069e4134d8e7\n",
      " ---> 28ba091bb396\n",
      "Step 4/10 : ENV HOME=/home\n",
      " ---> Running in edc354b2ee53\n",
      "Removing intermediate container edc354b2ee53\n",
      " ---> d61e64332e8f\n",
      "Step 5/10 : ENV PYTHONDONTWRITEBYTECODE=1\n",
      " ---> Running in 27a4b47111b8\n",
      "Removing intermediate container 27a4b47111b8\n",
      " ---> cf43875da1bb\n",
      "Step 6/10 : RUN rm -rf /var/sitecustomize\n",
      " ---> Running in 00c9f248d3d6\n",
      "Removing intermediate container 00c9f248d3d6\n",
      " ---> 9754982f5ebf\n",
      "Step 7/10 : COPY [\"./setup.py\", \"./setup.py\"]\n",
      " ---> 085874624eea\n",
      "Step 8/10 : RUN pip install --no-cache-dir .\n",
      " ---> Running in 57a71bc03eb7\n",
      "\u001b[91mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\n",
      "\u001b[0mProcessing /usr/app\n",
      "Collecting cloudml-hypertune\n",
      "  Downloading cloudml-hypertune-0.1.0.dev6.tar.gz (3.2 kB)\n",
      "Building wheels for collected packages: lending-club-hp-tuning, cloudml-hypertune\n",
      "  Building wheel for lending-club-hp-tuning (setup.py): started\n",
      "  Building wheel for lending-club-hp-tuning (setup.py): finished with status 'done'\n",
      "  Created wheel for lending-club-hp-tuning: filename=lending_club_hp_tuning-0.1-py2-none-any.whl size=1868 sha256=08b78375ad9a48dc5b4503ee25e8b39e4370aad74aa9aee83a700693a7e89264\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vICfne/wheels/09/02/81/71f0338b28a9e6cfbaf257c2db9e323f41927d282eb31d953f\n",
      "  Building wheel for cloudml-hypertune (setup.py): started\n",
      "  Building wheel for cloudml-hypertune (setup.py): finished with status 'done'\n",
      "  Created wheel for cloudml-hypertune: filename=cloudml_hypertune-0.1.0.dev6-py2.py3-none-any.whl size=5588 sha256=bcdea1d262081ba221be4acfe3a59e9db11e126f35bc9ba42c2d616ba0ebca2a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vICfne/wheels/cb/ae/07/58e2525740ad2ea506fce767d29272daaf2716babae89450c7\n",
      "Successfully built lending-club-hp-tuning cloudml-hypertune\n",
      "Installing collected packages: cloudml-hypertune, lending-club-hp-tuning\n",
      "Successfully installed cloudml-hypertune-0.1.0.dev6 lending-club-hp-tuning-0.1\n",
      "\u001b[91mWARNING: You are using pip version 20.1; however, version 20.3.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 57a71bc03eb7\n",
      " ---> 95c2659d0d12\n",
      "Step 9/10 : COPY [\"trainer\", \"trainer\"]\n",
      " ---> 7e9e9ecd1093\n",
      "Step 10/10 : ENTRYPOINT [\"python\", \"-m\", \"trainer.task\"]\n",
      " ---> Running in ba0bdb801fa5\n",
      "Removing intermediate container ba0bdb801fa5\n",
      " ---> 19970e00da28\n",
      "Successfully built 19970e00da28\n",
      "Successfully tagged gcr.io/mwpmltr/cloudai-autogenerated/lending_club_job_20210724_231846:20210725.04.18.46.881926\n",
      "\n",
      "A custom container image is built locally.\n",
      "\n",
      "The push refers to repository [gcr.io/mwpmltr/cloudai-autogenerated/lending_club_job_20210724_231846]\n",
      "85bd11e88629: Preparing\n",
      "f243a4339839: Preparing\n",
      "06a4028dbd03: Preparing\n",
      "344cf8721422: Preparing\n",
      "f5804000129c: Preparing\n",
      "4c57c3a2fee3: Preparing\n",
      "4c57c3a2fee3: Preparing\n",
      "296c56a6ee6b: Preparing\n",
      "2c438d738d60: Preparing\n",
      "12a5d56ad978: Preparing\n",
      "f2650c7a7c64: Preparing\n",
      "b1a9ba8760fd: Preparing\n",
      "f2650c7a7c64: Preparing\n",
      "ec03677c729a: Preparing\n",
      "78c3171c43e6: Preparing\n",
      "6ae1ebeae50a: Preparing\n",
      "c34a4d8294c8: Preparing\n",
      "8276fea1eb8b: Preparing\n",
      "a58c076686c8: Preparing\n",
      "48116b9cec2b: Preparing\n",
      "53fb146beab8: Preparing\n",
      "5f5d731f19ba: Preparing\n",
      "d559ad01f5c9: Preparing\n",
      "8944cb7f88be: Preparing\n",
      "7323907347ef: Preparing\n",
      "f1ebba1c1e58: Preparing\n",
      "e6708a930863: Preparing\n",
      "067985996699: Preparing\n",
      "d78b608be574: Preparing\n",
      "9ebddcddc12d: Preparing\n",
      "da8cdd8bbfd7: Preparing\n",
      "71725e24bfeb: Preparing\n",
      "f89fb4ac64eb: Preparing\n",
      "ed36ac5b8b67: Preparing\n",
      "8c2fd7b74873: Preparing\n",
      "1a36d078ca8f: Preparing\n",
      "e638ab181060: Preparing\n",
      "ebd667352663: Preparing\n",
      "f2773ea7b65a: Preparing\n",
      "bbb8bc7f0e93: Preparing\n",
      "7a7fcf89ef4a: Preparing\n",
      "a87311dc7421: Preparing\n",
      "5ae9bc7f38fb: Preparing\n",
      "9edaa71ce233: Preparing\n",
      "62fdddf6a67c: Preparing\n",
      "eff16de3ff64: Preparing\n",
      "61727f5e6796: Preparing\n",
      "f1ebba1c1e58: Waiting\n",
      "e6708a930863: Waiting\n",
      "067985996699: Waiting\n",
      "6ae1ebeae50a: Waiting\n",
      "d78b608be574: Waiting\n",
      "9ebddcddc12d: Waiting\n",
      "da8cdd8bbfd7: Waiting\n",
      "71725e24bfeb: Waiting\n",
      "c34a4d8294c8: Waiting\n",
      "8276fea1eb8b: Waiting\n",
      "f89fb4ac64eb: Waiting\n",
      "a58c076686c8: Waiting\n",
      "ed36ac5b8b67: Waiting\n",
      "48116b9cec2b: Waiting\n",
      "4c57c3a2fee3: Waiting\n",
      "8c2fd7b74873: Waiting\n",
      "296c56a6ee6b: Waiting\n",
      "2c438d738d60: Waiting\n",
      "1a36d078ca8f: Waiting\n",
      "53fb146beab8: Waiting\n",
      "5f5d731f19ba: Waiting\n",
      "e638ab181060: Waiting\n",
      "12a5d56ad978: Waiting\n",
      "ebd667352663: Waiting\n",
      "f2650c7a7c64: Waiting\n",
      "d559ad01f5c9: Waiting\n",
      "b1a9ba8760fd: Waiting\n",
      "8944cb7f88be: Waiting\n",
      "ec03677c729a: Waiting\n",
      "f2773ea7b65a: Waiting\n",
      "7323907347ef: Waiting\n",
      "78c3171c43e6: Waiting\n",
      "bbb8bc7f0e93: Waiting\n",
      "7a7fcf89ef4a: Waiting\n",
      "61727f5e6796: Waiting\n",
      "9edaa71ce233: Waiting\n",
      "a87311dc7421: Waiting\n",
      "62fdddf6a67c: Waiting\n",
      "eff16de3ff64: Waiting\n",
      "5ae9bc7f38fb: Waiting\n",
      "85bd11e88629: Pushed\n",
      "f5804000129c: Pushed\n",
      "344cf8721422: Pushed\n",
      "f243a4339839: Pushed\n",
      "4c57c3a2fee3: Layer already exists\n",
      "12a5d56ad978: Layer already exists\n",
      "06a4028dbd03: Pushed\n",
      "296c56a6ee6b: Layer already exists\n",
      "2c438d738d60: Layer already exists\n",
      "f2650c7a7c64: Layer already exists\n",
      "b1a9ba8760fd: Layer already exists\n",
      "ec03677c729a: Layer already exists\n",
      "6ae1ebeae50a: Layer already exists\n",
      "78c3171c43e6: Layer already exists\n",
      "c34a4d8294c8: Layer already exists\n",
      "8276fea1eb8b: Layer already exists\n",
      "a58c076686c8: Layer already exists\n",
      "48116b9cec2b: Layer already exists\n",
      "5f5d731f19ba: Layer already exists\n",
      "53fb146beab8: Layer already exists\n",
      "8944cb7f88be: Layer already exists\n",
      "d559ad01f5c9: Layer already exists\n",
      "7323907347ef: Layer already exists\n",
      "f1ebba1c1e58: Layer already exists\n",
      "067985996699: Layer already exists\n",
      "d78b608be574: Layer already exists\n",
      "e6708a930863: Layer already exists\n",
      "da8cdd8bbfd7: Layer already exists\n",
      "71725e24bfeb: Layer already exists\n",
      "f89fb4ac64eb: Layer already exists\n",
      "ed36ac5b8b67: Layer already exists\n",
      "9ebddcddc12d: Layer already exists\n",
      "8c2fd7b74873: Layer already exists\n",
      "1a36d078ca8f: Layer already exists\n",
      "e638ab181060: Layer already exists\n",
      "ebd667352663: Layer already exists\n",
      "f2773ea7b65a: Layer already exists\n",
      "bbb8bc7f0e93: Layer already exists\n",
      "7a7fcf89ef4a: Layer already exists\n",
      "a87311dc7421: Layer already exists\n",
      "5ae9bc7f38fb: Layer already exists\n",
      "9edaa71ce233: Layer already exists\n",
      "62fdddf6a67c: Layer already exists\n",
      "61727f5e6796: Layer already exists\n",
      "eff16de3ff64: Layer already exists\n",
      "20210725.04.18.46.881926: digest: sha256:9254baccc0ff6c46193e9a8be3122e76bb6dbadb2596781f36537dfe9ee65bff size: 10128\n",
      "\n",
      "Custom container image [gcr.io/mwpmltr/cloudai-autogenerated/lending_club_job_20210724_231846:20210725.04.18.46.881926] is created for your custom job.\n",
      "\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.beta.ai.custom-jobs.create) INVALID_ARGUMENT: Invalid JSON payload received. Unknown name \"maxTrialCount\" at 'custom_job.job_spec': Cannot find field.\n",
      "Invalid JSON payload received. Unknown name \"parallelTrialCount\" at 'custom_job.job_spec': Cannot find field.\n",
      "Invalid JSON payload received. Unknown name \"studySpec\" at 'custom_job.job_spec': Cannot find field.\n",
      "Invalid JSON payload received. Unknown name \"trialJobSpec\" at 'custom_job.job_spec': Cannot find field.\n",
      "- '@type': type.googleapis.com/google.rpc.BadRequest\n",
      "  fieldViolations:\n",
      "  - description: \"Invalid JSON payload received. Unknown name \\\"maxTrialCount\\\" at\\\n",
      "      \\ 'custom_job.job_spec': Cannot find field.\"\n",
      "    field: custom_job.job_spec\n",
      "  - description: \"Invalid JSON payload received. Unknown name \\\"parallelTrialCount\\\"\\\n",
      "      \\ at 'custom_job.job_spec': Cannot find field.\"\n",
      "    field: custom_job.job_spec\n",
      "  - description: \"Invalid JSON payload received. Unknown name \\\"studySpec\\\" at 'custom_job.job_spec':\\\n",
      "      \\ Cannot find field.\"\n",
      "    field: custom_job.job_spec\n",
      "  - description: \"Invalid JSON payload received. Unknown name \\\"trialJobSpec\\\" at\\\n",
      "      \\ 'custom_job.job_spec': Cannot find field.\"\n",
      "    field: custom_job.job_spec\n"
     ]
    }
   ],
   "source": [
    "now=(datetime.now() + timedelta(hours=-5)).strftime(\"%Y%m%d_%H%M%S\") # Central Time\n",
    "%env JOB_NAME=lending_club_job_{now}\n",
    "\n",
    "# This step will fail, but will create a container for training\n",
    "!gcloud beta ai custom-jobs create \\\n",
    "  --display-name $JOB_NAME \\\n",
    "  --config $HPTUNING_CONFIG \\\n",
    "  --region $REGION \\\n",
    "  --worker-pool-spec=executor-image-uri=$EXECUTOR_IMAGE_URI,local-package-path=$TRAINER_PACKAGE_PATH,python-module=$MAIN_TRAINER_MODULE,machine-type=n1-standard-4 \\\n",
    "  --args=project-id=$PROJECT_ID,bucket-name=$BUCKET_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "Hyperparameter tuning job [7361709735030030336] submitted successfully.\n",
      "\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud beta ai hp-tuning-jobs describe 7361709735030030336\n",
      "\n",
      "Job State: JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "!gcloud beta ai hp-tuning-jobs create \\\n",
    "  --display-name $JOB_NAME \\\n",
    "  --config $HPTUNING_CONFIG \\\n",
    "  --region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: JOB_NAME=7361709735030030336\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "JOB_STATE_RUNNING\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "JOB_STATE_RUNNING\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "JOB_STATE_RUNNING\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "JOB_STATE_RUNNING\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "JOB_STATE_RUNNING\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "JOB_STATE_RUNNING\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "JOB_STATE_RUNNING\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "JOB_STATE_RUNNING\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "JOB_STATE_RUNNING\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "JOB_STATE_RUNNING\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "JOB_STATE_RUNNING\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "JOB_STATE_RUNNING\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "JOB_STATE_RUNNING\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "JOB_STATE_RUNNING\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "JOB_STATE_RUNNING\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "JOB_STATE_RUNNING\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "JOB_STATE_RUNNING\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "JOB_STATE_RUNNING\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "JOB_STATE_RUNNING\n",
      "Using endpoint [https://us-central1-aiplatform.googleapis.com/]\n",
      "JOB_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "%env JOB_NAME=7361709735030030336\n",
    "# Model should exit with status \"SUCCEEDED\"\n",
    "# !gcloud ai-platform jobs describe $JOB_NAME --format=\"value(state)\"\n",
    "cmd = 'gcloud beta ai hp-tuning-jobs describe $JOB_NAME --region $REGION --format=\"value(state)\"'\n",
    "for i in range(20):\n",
    "    time.sleep(10)\n",
    "    !{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Optional] Part 4: StackDriver Logging\n",
    "You can view the logs for your training job:\n",
    "1. Go to https://console.cloud.google.com/\n",
    "1. Select \"Logging\" in left-hand pane\n",
    "1. In left-hand pane, go to \"AI Platform\" and select Jobs\n",
    "1. In filter by prefix, use the value of $JOB_NAME to view the logs\n",
    "\n",
    "On the logging page of your model, you can view the different results for each HP tuning job. \n",
    "\n",
    "Example:\n",
    "```\n",
    "{\n",
    "  \"trialId\": \"15\",\n",
    "  \"hyperparameters\": {\n",
    "    \"booster\": \"dart\",\n",
    "    \"max_depth\": \"7\",\n",
    "    \"n_estimators\": \"102\"\n",
    "  },\n",
    "  \"finalMetric\": {\n",
    "    \"trainingStep\": \"1000\",\n",
    "    \"objectiveValue\": 0.9259230441279733\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Optional] Part 5: Verify Model File in GCS\n",
    "View the contents of the destination model folder to verify that all 5 model files have indeed been uploaded to GCS.\n",
    "\n",
    "Note: The model can take a few minutes to train and show up in GCS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of the log output at the end of hyperparameter training showing the final AUC score and parameters chosen to achieve it:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "finalMetric:\n",
    "  trainingStep: '1000'\n",
    "  objectiveValue: 0.828859\n",
    "hyperparameters:\n",
    "  booster: gblinear\n",
    "  max_depth: '3'\n",
    "  num_boost_round: '112'\n",
    "startTime: '2020-04-06T16:34:25.152556682Z'\n",
    "state: SUCCEEDED\n",
    "trialId: '12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m75",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m75"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
