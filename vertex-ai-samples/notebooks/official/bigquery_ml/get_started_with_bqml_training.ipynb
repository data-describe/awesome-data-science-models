{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copyright"
   },
   "outputs": [],
   "source": [
    "# Copyright 2022 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title:generic,gcp"
   },
   "source": [
    "# Get started with BigQuery ML Training\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/bigquery_ml/get_started_with_bqml_training.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "    <td>\n",
    "        <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/bigquery_ml/get_started_with_bqml_training.ipynb\">\n",
    "        <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "        </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/bigquery_ml/get_started_with_bqml_training.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "overview:mlops"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to use Vertex AI in production. This tutorial covers get started with BigQuery ML training.\n",
    "\n",
    "Learn more about [BigQuery ML](https://cloud.google.com/vertex-ai/docs/beginner/bqml)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:mlops,stage2,get_started_bqml_training"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to use `BigQueryML` for training with `Vertex AI`.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- `BigQueryML Training`\n",
    "- `Vertex AI Model resource`\n",
    "- `Vertex AI Vizier`\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Create a local BigQuery table in your project\n",
    "- Train a BigQuery ML model\n",
    "- Evaluate the BigQuery ML model\n",
    "- Export the BigQuery ML model as a cloud model\n",
    "- Upload the exported model as a `Vertex AI Model` resource\n",
    "- Hyperparameter tune a BigQuery ML model with `Vertex AI Vizier`\n",
    "- Automatically register a BigQuery ML model to `Vertex AI Model Registry`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:penguins,lcn,bq"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "The dataset used for this tutorial is the Penguins dataset from [BigQuery public datasets](https://cloud.google.com/bigquery/public-data). This version of the dataset is used to predict the species of penguins from the available features like culmen-length, flipper-depth etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81c777b8ad32"
   },
   "source": [
    "### Costs\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "- Vertex AI\n",
    "- Cloud Storage\n",
    "- BigQuery\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing) and [BigQuery pricing](https://cloud.google.com/bigquery/pricing) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install_mlops"
   },
   "source": [
    "## Installations\n",
    "\n",
    "Install the following packages for executing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "install_mlops"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m  WARNING: The script plasma_store is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "\n",
    "# # The Vertex AI Workbench Notebook product has specific requirements\n",
    "# IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "# IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
    "\n",
    "# # Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "# USER_FLAG = \"\"\n",
    "# if IS_WORKBENCH_NOTEBOOK:\n",
    "#     USER_FLAG = \"--user\"\n",
    "\n",
    "# # Install the packages\n",
    "# ! pip3 install --upgrade pyarrow \\\n",
    "#                         google-cloud-aiplatform \\\n",
    "#                         google-cloud-bigquery \\\n",
    "#                         google-cloud-bigquery-storage \\\n",
    "#                         db-dtypes $USER_FLAG -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "restart"
   },
   "source": [
    "### Restart the kernel\n",
    "\n",
    "Once you've installed the additional packages, you need to restart the notebook kernel so it can find the packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "restart"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# if not os.getenv(\"IS_TESTING\"):\n",
    "#     # Automatically restart kernel after installs\n",
    "#     import IPython\n",
    "\n",
    "#     app = IPython.Application.instance()\n",
    "#     app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "project_id"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "1. [Enable the Vertex AI, BigQuery, Compute Engine and Cloud Storage APIs](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com,bigquery,compute_component,storage_component).\n",
    "\n",
    "1. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "1. Enter your project ID in the cell below. Then run the cell to make sure the\n",
    "Cloud SDK uses the right project for all the commands in this notebook.\n",
    "\n",
    "**Note**: Jupyter runs lines prefixed with `!` as shell commands, and it interpolates Python variables prefixed with `$` into these commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "56d591439df1"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, you may be able to get your project ID using `gcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "set_project_id"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "autoset_project_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: mwpmltr\n"
     ]
    }
   ],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = ! gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "set_gcloud_project_id"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "region"
   },
   "source": [
    "#### Region\n",
    "\n",
    "You can also change the `REGION` variable, which is used for operations\n",
    "throughout the rest of this notebook.  Below are regions supported for Vertex AI. We recommend that you choose the region closest to you.\n",
    "\n",
    "- Americas: `us-central1`\n",
    "- Europe: `europe-west4`\n",
    "- Asia Pacific: `asia-east1`\n",
    "\n",
    "You may not use a multi-regional bucket for training with Vertex AI. Not all regions provide support for all Vertex AI services.\n",
    "\n",
    "Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "region"
   },
   "outputs": [],
   "source": [
    "REGION = \"[your-region]\"  # @param {type: \"string\"}\n",
    "\n",
    "if REGION == \"[your-region]\":\n",
    "    REGION = \"us-central1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "timestamp"
   },
   "source": [
    "#### Timestamp\n",
    "\n",
    "If you are in a live tutorial session, you might be using a shared test account or project. To avoid name collisions between users on resources created, you create a timestamp for each instance session, and append the timestamp onto the name of resources you create in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "timestamp"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f3bd8c0d0469"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "**If you are using Vertex AI Workbench Notebooks**, your environment is already authenticated. Skip this step.\n",
    "\n",
    "**If you are using Colab**, run the cell below and follow the instructions when prompted to authenticate your account via oAuth.\n",
    "\n",
    "**Otherwise**, follow these steps:\n",
    "\n",
    "In the Cloud Console, go to the [Create service account key](https://console.cloud.google.com/apis/credentials/serviceaccountkey) page.\n",
    "\n",
    "1. **Click Create service account**.\n",
    "\n",
    "2. In the **Service account name** field, enter a name, and click **Create**.\n",
    "\n",
    "3. In the **Grant this service account access to project** section, click the Role drop-down list. Type \"Vertex AI\" into the filter box, and select **Vertex AI Administrator**. Type \"Storage Object Admin\" into the filter box, and select **Storage Object Admin**.\n",
    "\n",
    "4. Click Create. A JSON file that contains your key downloads to your local environment.\n",
    "\n",
    "5. Enter the path to your service account key as the GOOGLE_APPLICATION_CREDENTIALS variable in the cell below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "e0953a00668e"
   },
   "outputs": [],
   "source": [
    "# If you are running this notebook in Colab, run this cell and follow the\n",
    "# instructions to authenticate your GCP account. This provides access to your\n",
    "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
    "# requests.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# If on Vertex AI Workbench, then don't execute this code\n",
    "IS_COLAB = False\n",
    "if not os.path.exists(\"/opt/deeplearning/metadata/env_version\") and not os.getenv(\n",
    "    \"DL_ANACONDA_HOME\"\n",
    "):\n",
    "    if \"google.colab\" in sys.modules:\n",
    "        IS_COLAB = True\n",
    "        from google.colab import auth as google_auth\n",
    "\n",
    "        google_auth.authenticate_user()\n",
    "\n",
    "    # If you are running this notebook locally, replace the string below with the\n",
    "    # path to your service account key and run this cell to authenticate your GCP\n",
    "    # account.\n",
    "    elif not os.getenv(\"IS_TESTING\"):\n",
    "        %env GOOGLE_APPLICATION_CREDENTIALS ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bucket:mbsdk"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "When you initialize the Vertex SDK for Python, you specify a Cloud Storage staging bucket. The staging bucket is where all the data associated with your dataset and model resources are retained across sessions.\n",
    "\n",
    "Set the name of your Cloud Storage bucket below. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "UUID='0obvksai'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "bucket"
   },
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"[your-bucket-name]\"  # @param {type:\"string\"}\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "autoset_bucket"
   },
   "outputs": [],
   "source": [
    "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"[your-bucket-name]\":\n",
    "    BUCKET_NAME = PROJECT_ID + \"aip-\" + UUID\n",
    "BUCKET_URI = f\"gs://{BUCKET_NAME}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "create_bucket"
   },
   "source": [
    "**Only if your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_bucket"
   },
   "outputs": [],
   "source": [
    "# ! gsutil mb -l $REGION $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "validate_bucket"
   },
   "source": [
    "Finally, validate access to your Cloud Storage bucket by examining its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "validate_bucket"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 gs://mwpmltraip-0obvksai/artifacts/\n",
      "                                 gs://mwpmltraip-0obvksai/data/\n",
      "                                 gs://mwpmltraip-0obvksai/fraud-detection-model-path/\n",
      "                                 gs://mwpmltraip-0obvksai/penguins_model/\n",
      "                                 gs://mwpmltraip-0obvksai/pipeline_root/\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls -al $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "set_service_account"
   },
   "source": [
    "#### Service Account\n",
    "\n",
    "You use a service account to create Vertex AI Pipeline jobs. If you do not want to use your project's Compute Engine service account, set `SERVICE_ACCOUNT` to another service account ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "set_service_account"
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = \"[your-service-account]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "autoset_service_account"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Account: 55590906972-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"[your-service-account]\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = !gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    if IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "import_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "source": [
    "### Initialize Vertex AI and BigQuery SDKs for Python\n",
    "\n",
    "Initialize the Vertex AI SDK for Python for your project and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "init_aip:mbsdk"
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_bq"
   },
   "source": [
    "Create the BigQuery client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "init_bq"
   },
   "outputs": [],
   "source": [
    "bqclient = bigquery.Client(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "accelerators:prediction,mbsdk"
   },
   "source": [
    "### Set hardware accelerators\n",
    "\n",
    "You can set hardware accelerators for prediction.\n",
    "\n",
    "Set the variable `DEPLOY_GPU/DEPLOY_NGPU` to use a container image supporting a GPU and the number of GPUs allocated to the virtual machine (VM) instance. For example, to use a GPU container image with 4 Nvidia Telsa K80 GPUs allocated to each VM, you would specify:\n",
    "\n",
    "    (aiplatform.AcceleratorType.NVIDIA_TESLA_K80, 4)\n",
    "\n",
    "Otherwise specify `(None, None)` to use a container image to run on a CPU.\n",
    "\n",
    "Learn more [here](https://cloud.google.com/vertex-ai/docs/general/locations#accelerators) hardware accelerator support for your region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "accelerators:prediction,mbsdk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.getenv(\"IS_TESTING_DEPLOY_GPU\"):\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (\n",
    "        aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
    "        int(os.getenv(\"IS_TESTING_DEPLOY_GPU\")),\n",
    "    )\n",
    "else:\n",
    "    # DEPLOY_GPU, DEPLOY_NGPU = (aiplatform.gapic.AcceleratorType.NVIDIA_TESLA_K80, 1)\n",
    "    DEPLOY_GPU, DEPLOY_NGPU = (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "container:prediction"
   },
   "source": [
    "### Set pre-built containers\n",
    "\n",
    "Set the pre-built Docker container image for prediction.\n",
    "\n",
    "- Set the variable `TF` to the TensorFlow version of the container image. For example, `2-1` would be version 2.1, and `1-15` would be version 1.15. The following list shows some of the pre-built images available:\n",
    "\n",
    "\n",
    "For the latest list, see [Pre-built containers for prediction](https://cloud.google.com/ai-platform-unified/docs/predictions/pre-built-containers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "container:prediction"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment: us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-5:latest None\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"IS_TESTING_TF\"):\n",
    "    TF = os.getenv(\"IS_TESTING_TF\")\n",
    "else:\n",
    "    TF = \"2.5\".replace(\".\", \"-\")\n",
    "\n",
    "if TF[0] == \"2\":\n",
    "    if DEPLOY_GPU:\n",
    "        DEPLOY_VERSION = \"tf2-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        DEPLOY_VERSION = \"tf2-cpu.{}\".format(TF)\n",
    "else:\n",
    "    if DEPLOY_GPU:\n",
    "        DEPLOY_VERSION = \"tf-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        DEPLOY_VERSION = \"tf-cpu.{}\".format(TF)\n",
    "\n",
    "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], DEPLOY_VERSION\n",
    ")\n",
    "\n",
    "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "machine:prediction"
   },
   "source": [
    "### Set machine type\n",
    "\n",
    "Next, set the machine type to use for prediction.\n",
    "\n",
    "- Set the variable `DEPLOY_COMPUTE` to configure the compute resources for the VM which is used for prediction.\n",
    " - `machine type`\n",
    "     - `n1-standard`: 3.75GB of memory per vCPU.\n",
    "     - `n1-highmem`: 6.5GB of memory per vCPU\n",
    "     - `n1-highcpu`: 0.9 GB of memory per vCPU\n",
    " - `vCPUs`: number of \\[2, 4, 8, 16, 32, 64, 96 \\]\n",
    "\n",
    "*Note: You may also use n2 and e2 machine types for training and deployment, but they do not support GPUs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "machine:prediction"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploy machine type n1-standard-4\n"
     ]
    }
   ],
   "source": [
    "if os.getenv(\"IS_TESTING_DEPLOY_MACHINE\"):\n",
    "    MACHINE_TYPE = os.getenv(\"IS_TESTING_DEPLOY_MACHINE\")\n",
    "else:\n",
    "    MACHINE_TYPE = \"n1-standard\"\n",
    "\n",
    "VCPU = \"4\"\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCPU\n",
    "print(\"Deploy machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqml_intro"
   },
   "source": [
    "## BigQuery ML introduction\n",
    "\n",
    "BigQuery ML (BQML) provides the capability to train ML tabular models, such as classification and regression, in BigQuery using SQL syntax.\n",
    "\n",
    "Learn more about [BigQuery ML documentation](https://cloud.google.com/bigquery-ml/docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "import_file:penguins,bq,lcn"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mwpmltr.penguins.penguins'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMPORT_FILE = \"bq://bigquery-public-data.ml_datasets.penguins\"\n",
    "# BQ_TABLE = \"bigquery-public-data.ml_datasets.penguins\"\n",
    "BQ_TABLE = f\"{PROJECT_ID}.penguins.penguins\"\n",
    "BQ_TABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqml_create_dataset"
   },
   "source": [
    "### Create BQ dataset resource\n",
    "\n",
    "First, you create an empty dataset resource in your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "bqml_create_dataset"
   },
   "outputs": [],
   "source": [
    "BQ_DATASET_NAME = \"penguins\"\n",
    "DATASET_QUERY = f\"\"\"CREATE SCHEMA {BQ_DATASET_NAME}\n",
    "\"\"\"\n",
    "\n",
    "job = bqclient.query(DATASET_QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqml_create_model"
   },
   "source": [
    "### Train BigQuery ML model\n",
    "\n",
    "Next, you create and train a BigQuery ML tabular classification model from the public dataset penguins and store the model in your project using the `CREATE MODEL` statement. The model configuration is specified in the `OPTIONS` statement as follows:\n",
    "\n",
    "- `model_type`: The type and archictecture of tabular model to train, e.g., DNN classification.\n",
    "- `labels`: The column which are the labels.\n",
    "\n",
    "Learn more about [The CREATE MODEL statement](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created transfer config: projects/55590906972/locations/us-central1/transferConfigs/640f4dc6-0000-2ed8-aced-14c14ef8fda8\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "from google.cloud import bigquery_datatransfer, bigquery_datatransfer_v1\n",
    "\n",
    "transfer_client = bigquery_datatransfer.DataTransferServiceClient()\n",
    "\n",
    "destination_project_id = PROJECT_ID\n",
    "destination_dataset_id = BQ_DATASET_NAME\n",
    "source_project_id = \"bigquery-public-data\"\n",
    "source_dataset_id = \"ml_datasets\"\n",
    "transfer_config = bigquery_datatransfer.TransferConfig(\n",
    "    destination_dataset_id=destination_dataset_id,\n",
    "    display_name=\"Copy of bigquery-public-data ml_datasets\",\n",
    "    data_source_id=\"cross_region_copy\",\n",
    "    params={\n",
    "        \"source_project_id\": source_project_id,\n",
    "        \"source_dataset_id\": source_dataset_id,\n",
    "    },\n",
    "    # Run once.\n",
    "    schedule_options=bigquery_datatransfer_v1.types.ScheduleOptions(\n",
    "        start_time=datetime.now(timezone.utc),\n",
    "        end_time = datetime.now(timezone.utc) + timedelta(minutes=5)\n",
    "    )\n",
    "    \n",
    ")\n",
    "transfer_config = transfer_client.create_transfer_config(\n",
    "    parent=transfer_client.common_project_path(destination_project_id),\n",
    "    transfer_config=transfer_config,\n",
    ")\n",
    "print(f\"Created transfer config: {transfer_config.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "bqml_create_model"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None RUNNING\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "[{'reason': 'invalidQuery', 'location': 'query', 'message': 'The input data has NULL values in one or more columns: culmen_length_mm, culmen_depth_mm, flipper_length_mm, body_mass_g, sex. BQML automatically handles null values (See https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-auto-preprocessing#imputation). If null values represent a special value in the data, replace them with the desired value before training and then retry.'}] DONE\n",
      "penguins.penguins_model created in 0:08:55.764000\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"penguins_model\"\n",
    "MODEL_QUERY = f\"\"\"\n",
    "CREATE OR REPLACE MODEL `{BQ_DATASET_NAME}.{MODEL_NAME}`\n",
    "OPTIONS(\n",
    "    model_type='DNN_CLASSIFIER',\n",
    "    labels = ['species']\n",
    "    )\n",
    "AS\n",
    "SELECT *\n",
    "FROM `{BQ_TABLE}`\n",
    "\"\"\"\n",
    "\n",
    "job = bqclient.query(MODEL_QUERY)\n",
    "print(job.errors, job.state)\n",
    "\n",
    "while job.running():\n",
    "    from time import sleep\n",
    "\n",
    "    sleep(30)\n",
    "    print(\"Running ...\")\n",
    "print(job.errors, job.state)\n",
    "\n",
    "tblname = job.ddl_target_table\n",
    "tblname = \"{}.{}\".format(tblname.dataset_id, tblname.table_id)\n",
    "print(\"{} created in {}\".format(tblname, job.ended - job.started))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqml_eval_model"
   },
   "source": [
    "### Evaluate the trained BigQuery ML model\n",
    "\n",
    "Next, retrieve the model evaluation for the trained BigQuery ML model.\n",
    "\n",
    "Learn more about [The ML.EVALUATE function](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-evaluate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "bqml_eval_model"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   precision  recall  accuracy  f1_score  log_loss   roc_auc\n",
      "0   0.111389  0.1412  0.177326  0.124435  1.242311  0.388075\n"
     ]
    }
   ],
   "source": [
    "EVAL_QUERY = f\"\"\"\n",
    "SELECT *\n",
    "FROM\n",
    "  ML.EVALUATE(MODEL {BQ_DATASET_NAME}.{MODEL_NAME})\n",
    "ORDER BY  roc_auc desc\n",
    "LIMIT 1\"\"\"\n",
    "\n",
    "job = bqclient.query(EVAL_QUERY)\n",
    "results = job.result().to_dataframe()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqml_export_model"
   },
   "source": [
    "### Export the model from BigQuery ML\n",
    "\n",
    "The model you trained in BigQuery ML is a TensorFlow model. Next, you export the TensorFlow model artifacts in TF.SavedModel format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "bqml_export_model"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r1762e54448a27c_00000186eae730f6_1 ... (4s) Current status: DONE   \n",
      "CommandException: One or more URLs matched no objects.\n"
     ]
    }
   ],
   "source": [
    "param = f\"{PROJECT_ID}:{BQ_DATASET_NAME}.{MODEL_NAME} {BUCKET_URI}/{MODEL_NAME}\"\n",
    "! bq extract -m $param\n",
    "\n",
    "MODEL_DIR = f\"{BUCKET_URI}/{MODEL_NAME}\"\n",
    "! gsutil ls $MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://mwpmltraip-0obvksai/penguins_model/checkpoint\n",
      "gs://mwpmltraip-0obvksai/penguins_model/explanation_metadata.json\n",
      "gs://mwpmltraip-0obvksai/penguins_model/graph.pbtxt\n",
      "gs://mwpmltraip-0obvksai/penguins_model/model.ckpt-20.data-00000-of-00002\n",
      "gs://mwpmltraip-0obvksai/penguins_model/model.ckpt-20.data-00001-of-00002\n",
      "gs://mwpmltraip-0obvksai/penguins_model/model.ckpt-20.index\n",
      "gs://mwpmltraip-0obvksai/penguins_model/model.ckpt-20.meta\n",
      "gs://mwpmltraip-0obvksai/penguins_model/saved_model.pb\n",
      "gs://mwpmltraip-0obvksai/penguins_model/assets/\n",
      "gs://mwpmltraip-0obvksai/penguins_model/variables/\n"
     ]
    }
   ],
   "source": [
    "! gsutil ls $MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_bqml_model"
   },
   "source": [
    "## Upload the BigQuery ML model to a Vertex AI Model resource\n",
    "\n",
    "Finally, now that you have the BigQuery ML model exported, you upload the model artifacts to Vertex AI Model resource, in the same way as if you were uploading a custom trained model.\n",
    "\n",
    "Below is a partial list of mapping BigQuery ML model types to their corresponding exported model format:\n",
    "\n",
    "'LINEAR_REG'<br/>\n",
    "'LOGISTIC_REG' --> TensorFlow SavedFormat\n",
    "\n",
    "'AUTOML_CLASSIFIER'<br/>\n",
    "'AUTOML_REGRESSOR' --> TensorFlow SavedFormat\n",
    "\n",
    "'BOOSTED_TREE_CLASSIFIER'<br/>\n",
    "'BOOSTED_TREE_REGRESSOR' --> XGBoost format\n",
    "\n",
    "'DNN_CLASSIFIER'<br/>\n",
    "'DNN_REGRESSOR'<br/>\n",
    "'DNN_LINEAR_COMBINED_CLASSIFIER'<br/>\n",
    "'DNN_LINEAR_COMBINED_REGRESSOR' --> TensorFlow Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "upload_bqml_model"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/55590906972/locations/us-central1/models/7321852438523150336/operations/7482847122950193152\n",
      "Model created. Resource name: projects/55590906972/locations/us-central1/models/7321852438523150336@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/55590906972/locations/us-central1/models/7321852438523150336@1')\n"
     ]
    }
   ],
   "source": [
    "model = aiplatform.Model.upload(\n",
    "    display_name=\"penguins_\" + TIMESTAMP,\n",
    "    artifact_uri=MODEL_DIR,\n",
    "    serving_container_image_uri=DEPLOY_IMAGE,\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deploy_model:mbsdk,all"
   },
   "source": [
    "## Deploy the model\n",
    "\n",
    "Next, deploy your model for online prediction. To deploy the model, you invoke the `deploy` method, with the following parameters:\n",
    "\n",
    "- `deployed_model_display_name`: A human readable name for the deployed model.\n",
    "- `traffic_split`: Percent of traffic at the endpoint that goes to this model, which is specified as a dictionary of one or more key/value pairs.\n",
    "If only one model, then specify as { \"0\": 100 }, where \"0\" refers to this model being uploaded and 100 means 100% of the traffic.\n",
    "If there are existing models on the endpoint, for which the traffic needs to be split, then use model_id to specify as { \"0\": percent, model_id: percent, ... }, where model_id is the model id of an existing model to the deployed endpoint. The percents must add up to 100.\n",
    "- `machine_type`: The type of machine to use for training.\n",
    "- `accelerator_type`: The hardware accelerator type.\n",
    "- `accelerator_count`: The number of accelerators to attach to a worker replica.\n",
    "- `starting_replica_count`: The number of compute instances to initially provision.\n",
    "- `max_replica_count`: The maximum number of compute instances to scale to. In this tutorial, only one instance is provisioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "deploy_model:mbsdk,all"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/55590906972/locations/us-central1/endpoints/3285459491050487808/operations/5879565655606296576\n",
      "Endpoint created. Resource name: projects/55590906972/locations/us-central1/endpoints/3285459491050487808\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/55590906972/locations/us-central1/endpoints/3285459491050487808')\n",
      "Deploying model to Endpoint : projects/55590906972/locations/us-central1/endpoints/3285459491050487808\n",
      "Deploy Endpoint model backing LRO: projects/55590906972/locations/us-central1/endpoints/3285459491050487808/operations/3937388316302770176\n",
      "Endpoint model deployed. Resource name: projects/55590906972/locations/us-central1/endpoints/3285459491050487808\n"
     ]
    }
   ],
   "source": [
    "DEPLOYED_NAME = \"penguins-\" + TIMESTAMP\n",
    "\n",
    "TRAFFIC_SPLIT = {\"0\": 100}\n",
    "\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "if DEPLOY_GPU:\n",
    "    endpoint = model.deploy(\n",
    "        deployed_model_display_name=DEPLOYED_NAME,\n",
    "        traffic_split=TRAFFIC_SPLIT,\n",
    "        machine_type=DEPLOY_COMPUTE,\n",
    "        accelerator_type=DEPLOY_GPU.name,\n",
    "        accelerator_count=DEPLOY_NGPU,\n",
    "        min_replica_count=MIN_NODES,\n",
    "        max_replica_count=MAX_NODES,\n",
    "    )\n",
    "else:\n",
    "    endpoint = model.deploy(\n",
    "        deployed_model_display_name=DEPLOYED_NAME,\n",
    "        traffic_split=TRAFFIC_SPLIT,\n",
    "        machine_type=DEPLOY_COMPUTE,\n",
    "        min_replica_count=MIN_NODES,\n",
    "        max_replica_count=MAX_NODES,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "undeploy_model:mbsdk"
   },
   "source": [
    "#### Undeploy the model\n",
    "\n",
    "When you are done doing predictions, you undeploy the model from the `Endpoint` resouce. This deprovisions all compute resources and ends billing for the deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "undeploy_model:mbsdk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undeploying Endpoint model: projects/55590906972/locations/us-central1/endpoints/3285459491050487808\n",
      "Undeploy Endpoint model backing LRO: projects/55590906972/locations/us-central1/endpoints/3285459491050487808/operations/1847718089202860032\n",
      "Endpoint model undeployed. Resource name: projects/55590906972/locations/us-central1/endpoints/3285459491050487808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x7f34f4493a50> \n",
       "resource name: projects/55590906972/locations/us-central1/endpoints/3285459491050487808"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint.undeploy_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_delete:mbsdk"
   },
   "source": [
    "#### Delete the `Vertex AI Model` resource\n",
    "\n",
    "The method 'delete()' deletes the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "model_delete:mbsdk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting Model : projects/55590906972/locations/us-central1/models/7321852438523150336\n",
      "Delete Model  backing LRO: projects/55590906972/locations/us-central1/operations/122839431919960064\n",
      "Model deleted. . Resource name: projects/55590906972/locations/us-central1/models/7321852438523150336\n"
     ]
    }
   ],
   "source": [
    "model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7890ae6f6410"
   },
   "source": [
    "### Delete the `BigQuery ML` model\n",
    "\n",
    "Next, delete the `BigQuery ML` instance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "f0b6163e70c0"
   },
   "outputs": [],
   "source": [
    "MODEL_QUERY = f\"\"\"\n",
    "DROP MODEL `{BQ_DATASET_NAME}.{MODEL_NAME}`\n",
    "\"\"\"\n",
    "\n",
    "job = bqclient.query(MODEL_QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqml_create_model:vizier"
   },
   "source": [
    "### Hyperparameter Tune and train a BigQuery ML model\n",
    "\n",
    "Next, you train a BigQuery ML tabular classification model with hyperparameter tuning using the `Vertex AI Vizier` service. The hyperparameter settings are specified in the `OPTIONS` statement as follows:\n",
    "\n",
    "- `HPARAM_TUNING_ALGORITHM`: The algorithm for selecting the next trial parameters.\n",
    "- `num_trials`: The number of trials.\n",
    "- `max_parallel_trials`: The number of trials to do in parallel.\n",
    "\n",
    "Learn more about [Hyperparameter tuning for CREATE MODEL statements](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-hyperparameter-tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bqml_create_model:vizier"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"penguins_model\"\n",
    "MODEL_QUERY = f\"\"\"\n",
    "CREATE OR REPLACE MODEL `{BQ_DATASET_NAME}.{MODEL_NAME}`\n",
    "OPTIONS(\n",
    "    model_type='DNN_CLASSIFIER',\n",
    "    labels = ['species'],\n",
    "    num_trials=4,\n",
    "    max_parallel_trials=2,\n",
    "    HPARAM_TUNING_ALGORITHM = 'VIZIER_DEFAULT'\n",
    "    )\n",
    "AS\n",
    "SELECT *\n",
    "FROM `{BQ_TABLE}`\n",
    "\"\"\"\n",
    "\n",
    "job = bqclient.query(MODEL_QUERY)\n",
    "print(job.errors, job.state)\n",
    "\n",
    "while job.running():\n",
    "    from time import sleep\n",
    "\n",
    "    sleep(30)\n",
    "    print(\"Running ...\")\n",
    "print(job.errors, job.state)\n",
    "\n",
    "tblname = job.ddl_target_table\n",
    "tblname = \"{}.{}\".format(tblname.dataset_id, tblname.table_id)\n",
    "print(\"{} created in {}\".format(tblname, job.ended - job.started))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqml_eval_model"
   },
   "source": [
    "### Evaluate the BigQuery ML trained model\n",
    "\n",
    "Next, retrieve the model evaluation results for the trained BigQuery ML model.\n",
    "\n",
    "Learn more about [The ML.EVALUATE function](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-evaluate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "bqml_eval_model"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   precision  recall  accuracy  f1_score  log_loss   roc_auc\n",
      "0   0.111389  0.1412  0.177326  0.124435  1.242311  0.388075\n"
     ]
    }
   ],
   "source": [
    "EVAL_QUERY = f\"\"\"\n",
    "SELECT *\n",
    "FROM\n",
    "  ML.EVALUATE(MODEL {BQ_DATASET_NAME}.{MODEL_NAME})\n",
    "ORDER BY  roc_auc desc\n",
    "LIMIT 1\"\"\"\n",
    "\n",
    "job = bqclient.query(EVAL_QUERY)\n",
    "results = job.result().to_dataframe()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3f3cee1236b1"
   },
   "source": [
    "### Delete the `BigQuery ML` model\n",
    "\n",
    "Next, delete the `BigQuery ML` instance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "957b7d841502"
   },
   "outputs": [],
   "source": [
    "MODEL_QUERY = f\"\"\"\n",
    "DROP MODEL `{BQ_DATASET_NAME}.{MODEL_NAME}`\n",
    "\"\"\"\n",
    "\n",
    "job = bqclient.query(MODEL_QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqml_create_model:xai"
   },
   "source": [
    "### Train a BigQuery ML model with Explainability\n",
    "\n",
    "Next, you train the same BigQuery ML model, but this time you enable Vertex AI Explainability on the model predictions by adding the option:\n",
    "\n",
    "- `ENABLE_GLOBAL_EXPLAIN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "bqml_create_model:xai"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None RUNNING\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "[{'reason': 'invalidQuery', 'location': 'query', 'message': 'The input data has NULL values in one or more columns: culmen_length_mm, culmen_depth_mm, flipper_length_mm, body_mass_g, sex. BQML automatically handles null values (See https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-auto-preprocessing#imputation). If null values represent a special value in the data, replace them with the desired value before training and then retry.'}] DONE\n",
      "penguins.penguins_model created in 0:10:18.749000\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"penguins_model\"\n",
    "MODEL_QUERY = f\"\"\"\n",
    "CREATE OR REPLACE MODEL `{BQ_DATASET_NAME}.{MODEL_NAME}`\n",
    "OPTIONS(\n",
    "    model_type='DNN_CLASSIFIER',\n",
    "    labels = ['species'],\n",
    "    ENABLE_GLOBAL_EXPLAIN = True\n",
    "    )\n",
    "AS\n",
    "SELECT *\n",
    "FROM `{BQ_TABLE}`\n",
    "\"\"\"\n",
    "\n",
    "job = bqclient.query(MODEL_QUERY)\n",
    "print(job.errors, job.state)\n",
    "\n",
    "while job.running():\n",
    "    from time import sleep\n",
    "\n",
    "    sleep(30)\n",
    "    print(\"Running ...\")\n",
    "print(job.errors, job.state)\n",
    "\n",
    "tblname = job.ddl_target_table\n",
    "tblname = \"{}.{}\".format(tblname.dataset_id, tblname.table_id)\n",
    "print(\"{} created in {}\".format(tblname, job.ended - job.started))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4def8aaf3398"
   },
   "source": [
    "### Delete the `BigQuery ML` model\n",
    "\n",
    "Next, delete the `BigQuery ML` instance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "ff5b32618018"
   },
   "outputs": [],
   "source": [
    "MODEL_QUERY = f\"\"\"\n",
    "DROP MODEL `{BQ_DATASET_NAME}.{MODEL_NAME}`\n",
    "\"\"\"\n",
    "\n",
    "job = bqclient.query(MODEL_QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2b4498ca6fea"
   },
   "source": [
    "## Model Registry\n",
    "\n",
    "Alternatively, you can implicitly upload your BigQuery ML model as a `Vertex AI Model` resource with exporting and importing the model artifacts. In this method, you add additional options when training the model that tells BigQuery ML to automatically upload and register the trained model as a `Model` resource.\n",
    "\n",
    "### Setting permissions to automatically register the model\n",
    "\n",
    "You need to set some additional IAM permissions for BigQuery ML to automatically upload and register the model after training. Depending on your service account, the setting of the permissions below may fail. In this case, we recommend executing the permissions in a Cloud Shell.\n",
    "\n",
    "Learn more about [Setting permissions for Model Registry](https://cloud.google.com/bigquery-ml/docs/managing-models-vertex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "29229f72d13d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated IAM policy for project [mwpmltr].\n",
      "bindings:\n",
      "- members:\n",
      "  - serviceAccount:55590906972-compute@developer.gserviceaccount.com\n",
      "  - serviceAccount:af-test@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:resume-classifier@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:rtcrichard@mwpmltr.iam.gserviceaccount.com\n",
      "  role: roles/aiplatform.admin\n",
      "- members:\n",
      "  - serviceAccount:resume-classifier@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-55590906972@gcp-sa-aiplatform-cc.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-55590906972@gcp-sa-aiplatform.iam.gserviceaccount.com\n",
      "  role: roles/aiplatform.customCodeServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcp-sa-aiplatform-cc.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-55590906972@gcp-sa-aiplatform.iam.gserviceaccount.com\n",
      "  role: roles/aiplatform.serviceAgent\n",
      "- members:\n",
      "  - user:brian.ray@mavenwave.com\n",
      "  - user:ryan.russon@ex.mavenwave.com\n",
      "  role: roles/aiplatform.user\n",
      "- members:\n",
      "  - user:nina.ayar@mavenwave.com\n",
      "  role: roles/aiplatform.viewer\n",
      "- members:\n",
      "  - serviceAccount:face-search-demo@mwpmltr.iam.gserviceaccount.com\n",
      "  role: roles/appengine.appAdmin\n",
      "- members:\n",
      "  - user:charlie.sanders@ex.mavenwave.com\n",
      "  - user:lars.feste@mavenwave.com\n",
      "  role: roles/appengine.appViewer\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcp-gae-service.iam.gserviceaccount.com\n",
      "  role: roles/appengine.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gae-api-prod.google.com.iam.gserviceaccount.com\n",
      "  role: roles/appengineflex.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:rtcrichard@mwpmltr.iam.gserviceaccount.com\n",
      "  role: roles/artifactregistry.admin\n",
      "- members:\n",
      "  - serviceAccount:rtcrichard@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-55590906972@gcp-sa-artifactregistry.iam.gserviceaccount.com\n",
      "  role: roles/artifactregistry.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:rtcrichard@mwpmltr.iam.gserviceaccount.com\n",
      "  role: roles/artifactregistry.writer\n",
      "- members:\n",
      "  - user:sumeet.singh@mavenwave.com\n",
      "  role: roles/automl.admin\n",
      "- members:\n",
      "  - serviceAccount:custom-prediction-sa@mwpmltr.iam.gserviceaccount.com\n",
      "  - user:bart.calvin@mavenwave.com\n",
      "  role: roles/automl.predictor\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcp-sa-automl.iam.gserviceaccount.com\n",
      "  role: roles/automl.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:55590906972-compute@developer.gserviceaccount.com\n",
      "  - serviceAccount:service-55590906972@cloud-ml.google.com.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-55590906972@gcp-sa-healthcare.iam.gserviceaccount.com\n",
      "  - user:bobby.jacob@mavenwave.com\n",
      "  - user:ryan.russon@ex.mavenwave.com\n",
      "  role: roles/bigquery.admin\n",
      "- members:\n",
      "  - serviceAccount:looker-service-account@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:notebook-sa@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:rtcrichard@mwpmltr.iam.gserviceaccount.com\n",
      "  - user:sandeep.singh2@mavenwave.com\n",
      "  role: roles/bigquery.dataEditor\n",
      "- members:\n",
      "  - serviceAccount:looker-service-account@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:notebook-sa@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:rtcrichard@mwpmltr.iam.gserviceaccount.com\n",
      "  role: roles/bigquery.jobUser\n",
      "- members:\n",
      "  - user:girish.dhayalan@atos.net\n",
      "  role: roles/bigquery.user\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcp-sa-bigqueryconnection.iam.gserviceaccount.com\n",
      "  role: roles/bigqueryconnection.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcp-sa-bigquerydatatransfer.iam.gserviceaccount.com\n",
      "  role: roles/bigquerydatatransfer.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:55590906972@cloudbuild.gserviceaccount.com\n",
      "  role: roles/cloudbuild.builds.builder\n",
      "- members:\n",
      "  - serviceAccount:ml-dashboard-bitbucket-deploy@mwpmltr.iam.gserviceaccount.com\n",
      "  role: roles/cloudbuild.builds.editor\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcp-sa-cloudbuild.iam.gserviceaccount.com\n",
      "  role: roles/cloudbuild.serviceAgent\n",
      "- members:\n",
      "  - user:girish.dhayalan@atos.net\n",
      "  role: roles/cloudfunctions.developer\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcf-admin-robot.iam.gserviceaccount.com\n",
      "  role: roles/cloudfunctions.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcp-sa-cloudiot.iam.gserviceaccount.com\n",
      "  role: roles/cloudiot.serviceAgent\n",
      "- members:\n",
      "  - user:rishi.sheth@mavenwave.com\n",
      "  role: roles/cloudjobdiscovery.admin\n",
      "- members:\n",
      "  - user:bart.calvin@mavenwave.com\n",
      "  role: roles/cloudscheduler.admin\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcp-sa-cloudscheduler.iam.gserviceaccount.com\n",
      "  role: roles/cloudscheduler.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@cloudcomposer-accounts.iam.gserviceaccount.com\n",
      "  role: roles/composer.ServiceAgentV2Ext\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@cloudcomposer-accounts.iam.gserviceaccount.com\n",
      "  role: roles/composer.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:ml-dev-composer@mwpmltr.iam.gserviceaccount.com\n",
      "  role: roles/compute.admin\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@compute-system.iam.gserviceaccount.com\n",
      "  role: roles/compute.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:pg-gke-sa@mwpmltr.iam.gserviceaccount.com\n",
      "  - user:bobby.jacob@mavenwave.com\n",
      "  role: roles/container.admin\n",
      "- members:\n",
      "  - user:bobby.jacob@mavenwave.com\n",
      "  role: roles/container.clusterAdmin\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcp-sa-gkenode.iam.gserviceaccount.com\n",
      "  role: roles/container.nodeServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@container-engine-robot.iam.gserviceaccount.com\n",
      "  role: roles/container.serviceAgent\n",
      "- members:\n",
      "  - user:bobby.jacob@mavenwave.com\n",
      "  role: roles/containeranalysis.admin\n",
      "- members:\n",
      "  - user:bobby.jacob@mavenwave.com\n",
      "  role: roles/containeranalysis.notes.attacher\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcp-sa-ktd-control.iam.gserviceaccount.com\n",
      "  role: roles/containerthreatdetection.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:55590906972-compute@developer.gserviceaccount.com\n",
      "  - user:bobby.jacob@mavenwave.com\n",
      "  role: roles/dataflow.admin\n",
      "- members:\n",
      "  - user:girish.dhayalan@atos.net\n",
      "  role: roles/dataflow.developer\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@dataflow-service-producer-prod.iam.gserviceaccount.com\n",
      "  role: roles/dataflow.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcp-sa-datafusion.iam.gserviceaccount.com\n",
      "  role: roles/datafusion.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcp-sa-datalabeling.iam.gserviceaccount.com\n",
      "  role: roles/datalabeling.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@trifacta-gcloud-prod.iam.gserviceaccount.com\n",
      "  role: roles/dataprep.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@dataproc-accounts.iam.gserviceaccount.com\n",
      "  role: roles/dataproc.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:datastore-product-list@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:mlops-product-landscape-cicd@mwpmltr.iam.gserviceaccount.com\n",
      "  role: roles/datastore.user\n",
      "- members:\n",
      "  - serviceAccount:dialogflow-amohir@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:dialogflow-iwuaki@mwpmltr.iam.gserviceaccount.com\n",
      "  - user:cloud-contact-center-telephony@prod.google.com\n",
      "  role: roles/dialogflow.client\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcp-sa-dialogflow.iam.gserviceaccount.com\n",
      "  role: roles/dialogflow.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@dlp-api.iam.gserviceaccount.com\n",
      "  role: roles/dlp.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcp-sa-prod-dai-core.iam.gserviceaccount.com\n",
      "  role: roles/documentaicore.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:55590906972-compute@developer.gserviceaccount.com\n",
      "  - serviceAccount:55590906972@cloudservices.gserviceaccount.com\n",
      "  - serviceAccount:ds-platform-test@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:mw-data-describe@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:mwpmltr@appspot.gserviceaccount.com\n",
      "  - serviceAccount:nlp-api@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:notebook-users-a@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:pg-project-editor@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:retailtutorial-service-account@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-55590906972@containerregistry.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-acc-1678380045@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-acc-1678382412@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:vision-api@mwpmltr.iam.gserviceaccount.com\n",
      "  - user:alexander.bieniek@mavenwave.com\n",
      "  - user:ankur.2.srivastava@atos.net\n",
      "  - user:brian.ray@mavenwave.com\n",
      "  - user:claire.salling@mavenwave.com\n",
      "  - user:gurudatt.bhat@mavenwave.com\n",
      "  - user:jorge.duque@atos.net\n",
      "  - user:jose.ochoa@ex.mavenwave.com\n",
      "  - user:jothi.govindasamy@mavenwave.com\n",
      "  - user:kevin.yang@atos.net\n",
      "  - user:krithika.srinivasan@mavenwave.com\n",
      "  - user:manoj.kumar@mavenwave.com\n",
      "  - user:priyanka.bhatia@ex.mavenwave.com\n",
      "  - user:puneet.khera@mavenwave.com\n",
      "  - user:rishi.sheth@mavenwave.com\n",
      "  - user:rohit.gupta@mavenwave.com\n",
      "  - user:sahejpal.singh@mavenwave.com\n",
      "  - user:sandeep.singh2@mavenwave.com\n",
      "  - user:shel.davis@mavenwave.com\n",
      "  - user:shubhra.karmahe@mavenwave.com\n",
      "  - user:steven.pais@mavenwave.com\n",
      "  - user:zach.zhao@mavenwave.com\n",
      "  role: roles/editor\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@cloud-filer.iam.gserviceaccount.com\n",
      "  role: roles/file.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:firebase-service-account@firebase-sa-management.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-55590906972@gcp-sa-firebase.iam.gserviceaccount.com\n",
      "  role: roles/firebase.managementServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@firebase-rules.iam.gserviceaccount.com\n",
      "  role: roles/firebaserules.system\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcp-sa-firestore.iam.gserviceaccount.com\n",
      "  role: roles/firestore.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcp-sa-gkehub.iam.gserviceaccount.com\n",
      "  role: roles/gkehub.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcp-sa-healthcare.iam.gserviceaccount.com\n",
      "  role: roles/healthcare.serviceAgent\n",
      "- members:\n",
      "  - user:david.law@ex.mavenwave.com\n",
      "  role: roles/iam.roleAdmin\n",
      "- members:\n",
      "  - user:david.law@ex.mavenwave.com\n",
      "  role: roles/iam.serviceAccountAdmin\n",
      "- members:\n",
      "  - serviceAccount:55590906972@cloudbuild.gserviceaccount.com\n",
      "  - serviceAccount:rtcrichard@mwpmltr.iam.gserviceaccount.com\n",
      "  role: roles/iam.serviceAccountUser\n",
      "- members:\n",
      "  - user:david.law@ex.mavenwave.com\n",
      "  - user:sumeet.singh@mavenwave.com\n",
      "  role: roles/iap.httpsResourceAccessor\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcp-sa-krmapihosting.iam.gserviceaccount.com\n",
      "  role: roles/krmapihosting.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:mw-data-describe@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-55590906972@cloud-tpu.iam.gserviceaccount.com\n",
      "  role: roles/logging.logWriter\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcp-sa-meshdataplane.iam.gserviceaccount.com\n",
      "  role: roles/meshdataplane.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:rtcrichard@mwpmltr.iam.gserviceaccount.com\n",
      "  role: roles/ml.admin\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@cloud-ml.google.com.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-752662822785@cloud-tpu.iam.gserviceaccount.com\n",
      "  role: roles/ml.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcp-sa-monitoring-notification.iam.gserviceaccount.com\n",
      "  role: roles/monitoring.notificationServiceAgent\n",
      "- members:\n",
      "  - serviceAccount:notebook-sa@mwpmltr.iam.gserviceaccount.com\n",
      "  - user:sandeep.singh2@mavenwave.com\n",
      "  role: roles/notebooks.admin\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcp-sa-notebooks.iam.gserviceaccount.com\n",
      "  role: roles/notebooks.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:kubeflow-deploy-admin@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:openair2@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:openair3@mwpmltr.iam.gserviceaccount.com\n",
      "  - user:Jay.cheong@ex.mavenwave.com\n",
      "  - user:abadir.yimam@mavenwave.com\n",
      "  - user:alex.ji@ex.mavenwave.com\n",
      "  - user:anurag.bhatia@mavenwave.com\n",
      "  - user:arivarasu.perumal@mavenwave.com\n",
      "  - user:arsal.syed@mavenwave.com\n",
      "  - user:bobby.jacob@mavenwave.com\n",
      "  - user:brian.ray@mavenwave.com\n",
      "  - user:claire.salling@mavenwave.com\n",
      "  - user:daniel.dowler@mavenwave.com\n",
      "  - user:david.law@ex.mavenwave.com\n",
      "  - user:david.patterson@mavenwave.com\n",
      "  - user:jiwon.jeon@mavenwave.com\n",
      "  - user:lars.feste@mavenwave.com\n",
      "  - user:nicholas.beaudoin@mavenwave.com\n",
      "  - user:prashasta.gujrati@mavenwave.com\n",
      "  - user:rajeev.bajaj@mavenwave.com\n",
      "  - user:richard.truong-chau@mavenwave.com\n",
      "  - user:robert.annand@ex.mavenwave.com\n",
      "  - user:saurabh.gupta@mavenwave.com\n",
      "  - user:sumeet.singh@mavenwave.com\n",
      "  role: roles/owner\n",
      "- members:\n",
      "  - serviceAccount:pubsub-publisher@mwpmltr.iam.gserviceaccount.com\n",
      "  role: roles/pubsub.publisher\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@cloud-redis.iam.gserviceaccount.com\n",
      "  role: roles/redis.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:55590906972-compute@developer.gserviceaccount.com\n",
      "  - user:david.law@ex.mavenwave.com\n",
      "  role: roles/resourcemanager.projectIamAdmin\n",
      "- members:\n",
      "  - serviceAccount:55590906972-compute@developer.gserviceaccount.com\n",
      "  - serviceAccount:retailtutorial-service-account@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-acc-1678382412@mwpmltr.iam.gserviceaccount.com\n",
      "  - user:saurabh.gupta@mavenwave.com\n",
      "  role: roles/retail.admin\n",
      "- condition:\n",
      "    expression: resource.name == \"mwpmltr_cloudbuild\"\n",
      "    title: Cloud Build Default Logging Bucket\n",
      "  members:\n",
      "  - serviceAccount:service-acc-1678380045@mwpmltr.iam.gserviceaccount.com\n",
      "  role: roles/retail.admin\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcp-sa-retail.iam.gserviceaccount.com\n",
      "  role: roles/retail.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:55590906972@cloudbuild.gserviceaccount.com\n",
      "  role: roles/run.admin\n",
      "- members:\n",
      "  - serviceAccount:mlops-product-landscape-cicd@mwpmltr.iam.gserviceaccount.com\n",
      "  role: roles/run.developer\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@serverless-robot-prod.iam.gserviceaccount.com\n",
      "  role: roles/run.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:55590906972-compute@developer.gserviceaccount.com\n",
      "  - serviceAccount:resume-classifier@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-55590906972@gcp-sa-aiplatform-cc.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-55590906972@gcp-sa-aiplatform.iam.gserviceaccount.com\n",
      "  role: roles/secretmanager.admin\n",
      "- members:\n",
      "  - user:krithika.srinivasan@mavenwave.com\n",
      "  role: roles/secretmanager.secretAccessor\n",
      "- members:\n",
      "  - user:krithika.srinivasan@mavenwave.com\n",
      "  role: roles/secretmanager.secretVersionAdder\n",
      "- members:\n",
      "  - user:krithika.srinivasan@mavenwave.com\n",
      "  role: roles/secretmanager.secretVersionManager\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@service-networking.iam.gserviceaccount.com\n",
      "  role: roles/servicenetworking.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:ml-dashboard-bitbucket-deploy@mwpmltr.iam.gserviceaccount.com\n",
      "  role: roles/serviceusage.serviceUsageConsumer\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@sourcerepo-service-accounts.iam.gserviceaccount.com\n",
      "  role: roles/sourcerepo.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:af-test@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:face-search-demo@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:ml-dashboard-bitbucket-deploy@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:mwpmltr@appspot.gserviceaccount.com\n",
      "  - serviceAccount:notebook-sa@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:pg-gcr-sa@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:resume-classifier@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:rtcrichard@mwpmltr.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-55590906972@cloud-ml.google.com.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-55590906972@cloud-tpu.iam.gserviceaccount.com\n",
      "  - serviceAccount:service-account-iam-json@mwpmltr.iam.gserviceaccount.com\n",
      "  - user:ryan.russon@ex.mavenwave.com\n",
      "  role: roles/storage.admin\n",
      "- members:\n",
      "  - serviceAccount:rtcrichard@mwpmltr.iam.gserviceaccount.com\n",
      "  - user:david.law@ex.mavenwave.com\n",
      "  - user:sandeep.singh2@mavenwave.com\n",
      "  role: roles/storage.objectAdmin\n",
      "- condition:\n",
      "    expression: resource.name == \"mwpmltr_cloudbuild\"\n",
      "    title: Cloud Build Default Logging Bucket\n",
      "  members:\n",
      "  - serviceAccount:ml-dashboard-bitbucket-deploy@mwpmltr.iam.gserviceaccount.com\n",
      "  role: roles/storage.objectAdmin\n",
      "- members:\n",
      "  - serviceAccount:55590906972-compute@developer.gserviceaccount.com\n",
      "  - serviceAccount:mlops-product-landscape-cicd@mwpmltr.iam.gserviceaccount.com\n",
      "  role: roles/storage.objectCreator\n",
      "- members:\n",
      "  - serviceAccount:55590906972-compute@developer.gserviceaccount.com\n",
      "  - serviceAccount:rtcrichard@mwpmltr.iam.gserviceaccount.com\n",
      "  role: roles/storage.objectViewer\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@cloud-tpu.iam.gserviceaccount.com\n",
      "  role: roles/tpu.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:busi-test@mwpmltr.iam.gserviceaccount.com\n",
      "  - user:nina.ayar@mavenwave.com\n",
      "  role: roles/viewer\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcp-sa-vpcaccess.iam.gserviceaccount.com\n",
      "  role: roles/vpcaccess.serviceAgent\n",
      "- members:\n",
      "  - serviceAccount:service-55590906972@gcp-sa-websecurityscanner.iam.gserviceaccount.com\n",
      "  role: roles/websecurityscanner.serviceAgent\n",
      "etag: BwX3B0fMau4=\n",
      "version: 3\n"
     ]
    }
   ],
   "source": [
    "! gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "    --member=serviceAccount:$SERVICE_ACCOUNT --role=roles/aiplatform.admin --condition=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6c390ee7c11a"
   },
   "source": [
    "### Training and registering the model\n",
    "\n",
    "Next, you train the model and automatically register the model to the `Vertex AI Model Registry`, by adding the following parameters as options:\n",
    "\n",
    "- `model_registry`: Set to \"vertex_ai\" to indicate automatic registation to `Vertex AI Model Registry`.\n",
    "- `vertex_ai_model_id`: The human readable display name for the registered model.\n",
    "- `vertex_ai_model_version_aliases`: Alternate names for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "57db464f4c42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None RUNNING\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "Running ...\n",
      "[{'reason': 'invalidQuery', 'location': 'query', 'message': 'The input data has NULL values in one or more columns: culmen_length_mm, culmen_depth_mm, flipper_length_mm, body_mass_g, sex. BQML automatically handles null values (See https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-auto-preprocessing#imputation). If null values represent a special value in the data, replace them with the desired value before training and then retry.'}] DONE\n",
      "penguins.penguins_model created in 0:08:39.546000\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"penguins_model\"\n",
    "MODEL_QUERY = f\"\"\"\n",
    "CREATE OR REPLACE MODEL `{BQ_DATASET_NAME}.{MODEL_NAME}`\n",
    "OPTIONS(\n",
    "    model_type='DNN_CLASSIFIER',\n",
    "    labels = ['species'],\n",
    "    model_registry=\"vertex_ai\",\n",
    "    vertex_ai_model_id=\"bqml_model_{TIMESTAMP}\", \n",
    "    vertex_ai_model_version_aliases=[\"1\"]\n",
    "    )\n",
    "AS\n",
    "SELECT *\n",
    "FROM `{BQ_TABLE}`\n",
    "\"\"\"\n",
    "\n",
    "job = bqclient.query(MODEL_QUERY)\n",
    "print(job.errors, job.state)\n",
    "\n",
    "while job.running():\n",
    "    from time import sleep\n",
    "\n",
    "    sleep(30)\n",
    "    print(\"Running ...\")\n",
    "print(job.errors, job.state)\n",
    "\n",
    "tblname = job.ddl_target_table\n",
    "tblname = \"{}.{}\".format(tblname.dataset_id, tblname.table_id)\n",
    "print(\"{} created in {}\".format(tblname, job.ended - job.started))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b4970272040"
   },
   "source": [
    "### Find the model in the `Vertex Model Registry`\n",
    "\n",
    "Finally, you can use the `Vertex AI Model` list() method with a filter query to find the automatically registered model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "76c22674ba99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"projects/55590906972/locations/us-central1/models/bqml_model_20230316123105\"\n",
      "display_name: \"bqml_model_20230316123105\"\n",
      "supported_deployment_resources_types: DEDICATED_RESOURCES\n",
      "supported_input_storage_formats: \"jsonl\"\n",
      "supported_input_storage_formats: \"bigquery\"\n",
      "supported_input_storage_formats: \"csv\"\n",
      "supported_input_storage_formats: \"tf-record\"\n",
      "supported_input_storage_formats: \"tf-record-gzip\"\n",
      "supported_input_storage_formats: \"file-list\"\n",
      "supported_output_storage_formats: \"jsonl\"\n",
      "supported_output_storage_formats: \"bigquery\"\n",
      "create_time {\n",
      "  seconds: 1678985560\n",
      "  nanos: 758295000\n",
      "}\n",
      "update_time {\n",
      "  seconds: 1678985670\n",
      "  nanos: 593662000\n",
      "}\n",
      "etag: \"AMEw9yNe29lALfSRO5GHIhzAELoZYZ_pyQ5wZw1wQf_YTyaFhlq9sJ5PTR1m3Girv4HZ\"\n",
      "version_id: \"1\"\n",
      "version_aliases: \"1\"\n",
      "version_aliases: \"default\"\n",
      "version_create_time {\n",
      "  seconds: 1678985560\n",
      "  nanos: 758295000\n",
      "}\n",
      "version_update_time {\n",
      "  seconds: 1678986073\n",
      "  nanos: 716816000\n",
      "}\n",
      "model_source_info {\n",
      "  source_type: BQML\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = aiplatform.Model.list(filter=\"display_name=bqml_model_\" + TIMESTAMP)\n",
    "model = models[0]\n",
    "\n",
    "print(model.gca_resource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "48e6ef5d5ffa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bqml_model_20230316123105\n",
      "bqml_model_churn_0obvksai\n"
     ]
    }
   ],
   "source": [
    "models = aiplatform.Model.list()\n",
    "for model in models:\n",
    "    if model.gca_resource.display_name.startswith(\"bqml\"):\n",
    "        print(model.gca_resource.display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef61354b1a5f"
   },
   "source": [
    "### Delete the `BigQuery ML` model\n",
    "\n",
    "Next, delete the `BigQuery ML` instance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "f6004d1ce59d"
   },
   "outputs": [],
   "source": [
    "MODEL_QUERY = f\"\"\"\n",
    "DROP MODEL `{BQ_DATASET_NAME}.{MODEL_NAME}`\n",
    "\"\"\"\n",
    "\n",
    "job = bqclient.query(MODEL_QUERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "source": [
    "# Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial.\n",
    "\n",
    "Set `delete_storage` to `True` to delete the Cloud Storage bucket used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "cleanup:mbsdk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting Endpoint : projects/55590906972/locations/us-central1/endpoints/3285459491050487808\n",
      "Delete Endpoint  backing LRO: projects/55590906972/locations/us-central1/operations/6798862929543299072\n",
      "Endpoint deleted. . Resource name: projects/55590906972/locations/us-central1/endpoints/3285459491050487808\n",
      "Deleting Model : projects/55590906972/locations/us-central1/models/2763107904973176832\n",
      "Delete Model  backing LRO: projects/55590906972/locations/us-central1/operations/4785753896108687360\n",
      "Model deleted. . Resource name: projects/55590906972/locations/us-central1/models/2763107904973176832\n"
     ]
    }
   ],
   "source": [
    "# Delete the endpoint using the Vertex endpoint object\n",
    "endpoint.undeploy_all()\n",
    "endpoint.delete()\n",
    "\n",
    "# Delete the model using the Vertex model object\n",
    "try:\n",
    "    model.delete()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Delete the created BigQuery dataset\n",
    "! bq rm -r -f $PROJECT_ID:$BQ_DATASET_NAME\n",
    "\n",
    "delete_storage = False\n",
    "if delete_storage or os.getenv(\"IS_TESTING\"):\n",
    "    # Delete the created GCS bucket\n",
    "    ! gsutil rm -r $BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "get_started_with_bqml_training.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m103"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
