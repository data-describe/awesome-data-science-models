{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engineer Training Data\n",
    "\n",
    "Now that we have enriched the artist data with descriptive information about each artist, our next step is to join and refine the input data so that it can be used to train an AutoML classification model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_counts_df = pd.read_feather(\"input_data/play_counts_df.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_counts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pivot the table, using the user_name as the index. This groups the data by user_name and retrieves a count per user of the number of listens for each artist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_counts_pivot_df = pd.DataFrame(play_counts_df.pivot(index='user_name', columns='artist_name', values='cnt').to_records())\n",
    "play_counts_pivot_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check that this value is 20\n",
    "play_counts_pivot_df[play_counts_pivot_df.user_name == \"-nils-\"].iloc[0]['Muse']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the artist data for each user, we would also like to use information about the user's preferences for certain genres or styles of artist. To obtain features that represent this information, we're going to use the enriched artist data that was generated using the [1-Enrich_Raw_Input_Data.ipynb](./1-Enrich_Raw_Input_Data.ipynb) file.\n",
    "\n",
    "Our goal here is to create a count for each user of the total number of times a particular LastFM tag is represented in that user's listening history. To do this, we'll join the user data with the tag data using 'artist' as the join key, then sum the tag columns to get a count for the total number of times a particular tag shows up in a user's listening history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "band_tags = pd.read_feather(\"enriched_data/band_tags.feather\")\n",
    "band_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(band_tags.columns)[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "play_counts_with_tags_df = pd.merge(play_counts_df, band_tags, on='artist_name', how='left')\n",
    "# prevent data leakage by removing Beatles data from the tag df\n",
    "play_counts_with_tags_df_no_beatles = play_counts_with_tags_df[play_counts_with_tags_df['artist_name']!='The Beatles'].copy()\n",
    "user_tag_counts_df = play_counts_with_tags_df_no_beatles.drop(columns=['artist_name','cnt']).groupby('user_name').sum().reset_index()\n",
    "user_artist_tag_counts_df = pd.merge(play_counts_pivot_df, user_tag_counts_df, on='user_name', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_artist_tag_counts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a Target column to turn this into a classification problem, if they play Beatles (even once) then label is True. Then drop the column 'The Beatles' in order to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "user_artist_tag_counts_df['Like The Beatles'] =  user_artist_tag_counts_df['The Beatles'].apply(lambda x: not np.isnan(x))\n",
    "user_artist_tag_counts_df.drop('The Beatles', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to clean up the column names as Cloud AutoML is very picky and will fail if you do things like have Unicode file names. I wish Google will fix this. I'll complain ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_artist_tag_counts_df.columns = user_artist_tag_counts_df.columns.str.replace(\"post-roc\", \"post roc1\", regex=False)\n",
    "user_artist_tag_counts_df.columns = user_artist_tag_counts_df.columns.str.replace(\"trip-hop\", \"trip hop1\", regex=False)\n",
    "user_artist_tag_counts_df.columns = user_artist_tag_counts_df.columns.str.replace(\" \", \"_\", regex=False)\n",
    "user_artist_tag_counts_df.columns = user_artist_tag_counts_df.columns.str.replace(\"/\", \"\", regex=False)\n",
    "user_artist_tag_counts_df.columns = user_artist_tag_counts_df.columns.str.replace(\"é\", \"\", regex=False)\n",
    "user_artist_tag_counts_df.columns = user_artist_tag_counts_df.columns.str.replace(\"ö\", \"o\", regex=False)\n",
    "user_artist_tag_counts_df.columns = user_artist_tag_counts_df.columns.str.replace(\"+\", \"_and_\", regex=False)\n",
    "user_artist_tag_counts_df.columns = user_artist_tag_counts_df.columns.str.replace(\"&\", \"and\", regex=False)\n",
    "user_artist_tag_counts_df.columns = user_artist_tag_counts_df.columns.str.replace(\"!\", \"\", regex=False)\n",
    "user_artist_tag_counts_df.columns = user_artist_tag_counts_df.columns.str.replace(\"-\", \"_\", regex=False)\n",
    "user_artist_tag_counts_df.columns = user_artist_tag_counts_df.columns.str.replace(\".\", \"\", regex=False)\n",
    "user_artist_tag_counts_df.columns = user_artist_tag_counts_df.columns.str.replace(\"ó\", \"o\", regex=False)\n",
    "user_artist_tag_counts_df.columns = user_artist_tag_counts_df.columns.str.replace(\"'\", \"\", regex=False)\n",
    "\n",
    "# Japanese Artists\n",
    "user_artist_tag_counts_df.columns = user_artist_tag_counts_df.columns.str.replace(\"久石譲\", \"Joe_Hisaishi\", regex=False)\n",
    "user_artist_tag_counts_df.columns = user_artist_tag_counts_df.columns.str.replace(\"川井憲次\", \"Kenji_Kawai\", regex=False)\n",
    "user_artist_tag_counts_df.columns = user_artist_tag_counts_df.columns.str.replace(\"梶浦由記\", \"Yuki_Kajiura\", regex=False)\n",
    "user_artist_tag_counts_df.columns = user_artist_tag_counts_df.columns.str.replace(\"植松伸夫\", \"Nobuo_Uematsu\", regex=False)\n",
    "user_artist_tag_counts_df.columns = user_artist_tag_counts_df.columns.str.replace(\"菅野よう子\", \"Yoko_Kanno\", regex=False)\n",
    "user_artist_tag_counts_df.columns = user_artist_tag_counts_df.columns.str.replace(\"近藤浩治\", \"Koji_Kondo\", regex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set aside a random sample of data as a holdout set in order to test the trained and deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_artist_tag_counts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_artist_tag_counts_df.Like_The_Beatles.value_counts().plot(kind='pie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_artist_tag_counts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull 10 data points to test online model serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_sample = user_artist_tag_counts_df.sample(n=10, random_state=42)\n",
    "inference_sample.reset_index(drop=True).to_feather(\"test_data/inference_sample.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remainder = user_artist_tag_counts_df.drop(inference_sample.index)\n",
    "train_sample = remainder.sample(n=1985, random_state=42)\n",
    "remainder = remainder.drop(train_sample.index)\n",
    "validation_sample = remainder.sample(n=250, random_state=42)\n",
    "test_sample = remainder.drop(validation_sample.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample.loc[:, \"data_split\"] = \"TRAIN\"\n",
    "validation_sample.loc[:, \"data_split\"] = \"VALIDATE\"\n",
    "test_sample.loc[:, \"data_split\"] = \"TEST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that these numbers add up the way they should\n",
    "train_sample.shape[0] + validation_sample.shape[0] + test_sample.shape[0] + inference_sample.shape[0] == user_artist_tag_counts_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_df = pd.concat([train_sample, validation_sample, test_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_df.to_csv(\"training_data/file_out_2485_tags.csv\", index=None, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running head on the output CSV file will serve as a quick gut check to make sure that everything looks good in the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_df.data_split.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_df.data_split.value_counts() / len(input_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!head training_data/file_out_2485_tags.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line below copies the CSV file that was just output to a GCS bucket in Google Cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp training_data/file_out_2485_tags.csv gs://csalling-docai-datasets-regional/beatles/file_out_2485_tags.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that normally I wouldn't use a bucket obviously intended for a different application, but right now for some reason we can only create buckets that are in Finland, so I'm using an older bucket I had lying around for experimentation purposes."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m98",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m98"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
