{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7430fcab-d000-4c6a-ba10-7e9e8708277b",
   "metadata": {},
   "source": [
    "# Deploy & Run Online Inference\n",
    "\n",
    "Now that we've trained the model, and we are (presumably) happy with the results of training, we can deploy the model to a Vertex AI endpoint and use online predictions in order to test out a sample datapoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a81f76e5-7601-4c49-b580-105fb3243388",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = 'mwpmltr'\n",
    "LOCATION = \"us-central1\"\n",
    "MODEL_NAME = \"beatles_automl_file_out_2200_tags\"\n",
    "TARGET_COLUMN = \"Like_The_Beatles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35274ac6-fbd2-4138-95d6-6faea2b058ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dac1aaa-8b82-411c-bb25-e397631f6e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_endpoint(\n",
    "    project: str,\n",
    "    display_name: str,\n",
    "    location: str,\n",
    "):\n",
    "    \"\"\"Create an Vertex AI Model Endpoint in the given project and location\"\"\"\n",
    "    \n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name=display_name,\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "\n",
    "    print(endpoint.display_name)\n",
    "    print(endpoint.resource_name)\n",
    "    return endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed3b443a-a111-46cc-a6f1-ec4d9f645d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that you don't have to create an endpoint every time you run this notebook\n",
    "# create_endpoint(PROJECT_NAME, f'{MODEL_NAME}_endpoint', LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7e226cd-1690-43ae-bde1-8803a919ed14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    model_name: str,\n",
    "    endpoint_name: str\n",
    "):\n",
    "    \"\"\"\n",
    "    model_name: A fully-qualified model resource name or model ID.\n",
    "    endpoint_name: A fully-qualified endpoint resource name or endpoint ID.\n",
    "    \"\"\"\n",
    "\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    model = aiplatform.Model(model_name=model_name)\n",
    "    endpoint = aiplatform.Endpoint(endpoint_name=endpoint_name)\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=\"e2-standard-4\"\n",
    "    )\n",
    "\n",
    "    model.wait()\n",
    "\n",
    "    print(model.display_name)\n",
    "    print(model.resource_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a09d5b6-3fde-43b0-853b-f2cf3b282762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model to Endpoint : projects/55590906972/locations/us-central1/endpoints/3602110043756953600\n",
      "Deploy Endpoint model backing LRO: projects/55590906972/locations/us-central1/endpoints/3602110043756953600/operations/4494827517445668864\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_17813/3257557596.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mLOCATION\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"projects/55590906972/locations/us-central1/models/8993391587719380992\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;34m\"projects/55590906972/locations/us-central1/endpoints/3602110043756953600\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;32m/var/tmp/ipykernel_17813/363102711.py\u001b[0m in \u001b[0;36mdeploy_model\u001b[0;34m(project, location, model_name, endpoint_name)\u001b[0m\n\u001b[1;32m     17\u001b[0m     model.deploy(\n\u001b[1;32m     18\u001b[0m         \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mmachine_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"e2-standard-4\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, endpoint, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata, encryption_spec_key_name, network, sync, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle)\u001b[0m\n\u001b[1;32m   3316\u001b[0m             \u001b[0mdeploy_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeploy_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3317\u001b[0m             \u001b[0mautoscaling_target_cpu_utilization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoscaling_target_cpu_utilization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3318\u001b[0;31m             \u001b[0mautoscaling_target_accelerator_duty_cycle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoscaling_target_accelerator_duty_cycle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3319\u001b[0m         )\n\u001b[1;32m   3320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m                     \u001b[0mVertexAiResourceNounWithFutureManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36m_deploy\u001b[0;34m(self, endpoint, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_spec, metadata, encryption_spec_key_name, network, sync, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle)\u001b[0m\n\u001b[1;32m   3479\u001b[0m             \u001b[0mdeploy_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeploy_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3480\u001b[0m             \u001b[0mautoscaling_target_cpu_utilization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoscaling_target_cpu_utilization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3481\u001b[0;31m             \u001b[0mautoscaling_target_accelerator_duty_cycle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoscaling_target_accelerator_duty_cycle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3482\u001b[0m         )\n\u001b[1;32m   3483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36m_deploy_call\u001b[0;34m(cls, api_client, endpoint_resource_name, model, endpoint_resource_traffic_split, network, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_spec, metadata, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle)\u001b[0m\n\u001b[1;32m   1229\u001b[0m         )\n\u001b[1;32m   1230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1231\u001b[0;31m         \u001b[0moperation_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1233\u001b[0m     def undeploy(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \"\"\"\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocking_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36m_blocking_poll\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mpolling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_done_or_raise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRetryError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             raise concurrent.futures.TimeoutError(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m                 \u001b[0msleep_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                 \u001b[0mon_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_error\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m             )\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;34m\"Retrying due to {}, sleeping {:.1f}s ...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_exc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msleep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         )\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sleep generator stopped yielding sleep values.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# deploy_model(\n",
    "#     PROJECT_NAME,\n",
    "#     LOCATION,\n",
    "#     \"projects/55590906972/locations/us-central1/models/8993391587719380992\",\n",
    "#     \"projects/55590906972/locations/us-central1/endpoints/3602110043756953600\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c0aaac-db81-4d6f-8c0e-d40429140351",
   "metadata": {},
   "source": [
    "Now that the model is deployed to the prediction endpoint, we will use our test data point and make an API call to the Vertex AI online inference service, in order to predict whether this user would like the Beatles or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "118063b9-b95c-4a6d-b9e2-82e2efad9d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "\n",
    "def predict_tabular_classification(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    endpoint_name: str,\n",
    "    instances: List[Dict],\n",
    "):\n",
    "    \"\"\"\n",
    "    Args\n",
    "        project: Your project ID or project number.\n",
    "        location: Region where Endpoint is located. For example, 'us-central1'.\n",
    "        endpoint_name: A fully qualified endpoint name or endpoint ID. Example: \"projects/123/locations/us-central1/endpoints/456\" or\n",
    "               \"456\" when project and location are initialized or passed.\n",
    "        instances: A list of one or more instances (examples) to return a prediction for.\n",
    "    \"\"\"\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    endpoint = aiplatform.Endpoint(endpoint_name)\n",
    "\n",
    "    response = endpoint.predict(instances=instances)\n",
    "\n",
    "    for prediction_ in response.predictions:\n",
    "        print(prediction_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f86642cc-435f-40f1-bb21-e5d829412c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "inference_sample = pd.read_feather('test_data/inference_sample.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f29607a7-14ac-4f49-a9be-cb0a1d847894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62626de1-9b86-40ae-9db3-4e15af4b93c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_name</th>\n",
       "      <th>30_Seconds_to_Mars</th>\n",
       "      <th>65daysofstatic</th>\n",
       "      <th>A_Perfect_Circle</th>\n",
       "      <th>A_Tribe_Called_Quest</th>\n",
       "      <th>ABBA</th>\n",
       "      <th>ACDC</th>\n",
       "      <th>Adele</th>\n",
       "      <th>Aerosmith</th>\n",
       "      <th>Air</th>\n",
       "      <th>...</th>\n",
       "      <th>tag_shoegazer</th>\n",
       "      <th>tag_hair_metal</th>\n",
       "      <th>tag_rapcore</th>\n",
       "      <th>tag_underground_hip_hop</th>\n",
       "      <th>tag_symphonic_black_metal</th>\n",
       "      <th>tag_darkwave</th>\n",
       "      <th>tag_world</th>\n",
       "      <th>tag_latin</th>\n",
       "      <th>tag_spanish</th>\n",
       "      <th>Like_The_Beatles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thegiant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nezter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>augustohp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52.0</td>\n",
       "      <td>502.0</td>\n",
       "      <td>None</td>\n",
       "      <td>1.0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stalphonzo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>davenall</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Andy_Greenwell</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lilyean</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>absentbebnim</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>adherr</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>auserzz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 514 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_name  30_Seconds_to_Mars  65daysofstatic  A_Perfect_Circle  \\\n",
       "0        thegiant                 1.0             NaN               NaN   \n",
       "1          nezter                 NaN             NaN               NaN   \n",
       "2       augustohp                 NaN            52.0             502.0   \n",
       "3      stalphonzo                 NaN             NaN               NaN   \n",
       "4        davenall                 NaN             NaN               NaN   \n",
       "5  Andy_Greenwell                 NaN             NaN               NaN   \n",
       "6         lilyean                 NaN             NaN               NaN   \n",
       "7    absentbebnim                 NaN             NaN               NaN   \n",
       "8          adherr                 NaN             NaN               NaN   \n",
       "9         auserzz                 NaN             NaN               NaN   \n",
       "\n",
       "  A_Tribe_Called_Quest  ABBA   ACDC  Adele  Aerosmith   Air  ...  \\\n",
       "0                 None   NaN    NaN   11.0        1.0   NaN  ...   \n",
       "1                 None   NaN    NaN    NaN        NaN   3.0  ...   \n",
       "2                 None   1.0  452.0    1.0      215.0  14.0  ...   \n",
       "3                 None   NaN    6.0    NaN        NaN   NaN  ...   \n",
       "4                 None   NaN    NaN    NaN        NaN   NaN  ...   \n",
       "5                 None   NaN    NaN    NaN        NaN   NaN  ...   \n",
       "6                 None   NaN    NaN    NaN        NaN   NaN  ...   \n",
       "7                 None   NaN    NaN    NaN        NaN   NaN  ...   \n",
       "8                 None   NaN    NaN    NaN        NaN   NaN  ...   \n",
       "9                 None   NaN    NaN   25.0        NaN   NaN  ...   \n",
       "\n",
       "   tag_shoegazer  tag_hair_metal  tag_rapcore  tag_underground_hip_hop  \\\n",
       "0            0.0             0.0          0.0                      0.0   \n",
       "1            0.0             0.0          0.0                      1.0   \n",
       "2            0.0             2.0          1.0                      0.0   \n",
       "3            0.0             0.0          0.0                      0.0   \n",
       "4            0.0             0.0          0.0                      0.0   \n",
       "5            0.0             0.0          0.0                      0.0   \n",
       "6            0.0             0.0          0.0                      0.0   \n",
       "7            0.0             0.0          0.0                      0.0   \n",
       "8            0.0             0.0          0.0                      0.0   \n",
       "9            0.0             1.0          0.0                      0.0   \n",
       "\n",
       "   tag_symphonic_black_metal tag_darkwave  tag_world  tag_latin  tag_spanish  \\\n",
       "0                        0.0          0.0        0.0        0.0          0.0   \n",
       "1                        0.0          0.0        0.0        1.0          1.0   \n",
       "2                        0.0          0.0        0.0        1.0          1.0   \n",
       "3                        0.0          0.0        0.0        0.0          0.0   \n",
       "4                        0.0          0.0        0.0        0.0          0.0   \n",
       "5                        0.0          0.0        0.0        0.0          0.0   \n",
       "6                        0.0          0.0        0.0        0.0          0.0   \n",
       "7                        0.0          0.0        0.0        0.0          0.0   \n",
       "8                        0.0          0.0        0.0        0.0          0.0   \n",
       "9                        0.0          0.0        0.0        1.0          1.0   \n",
       "\n",
       "   Like_The_Beatles  \n",
       "0              True  \n",
       "1             False  \n",
       "2              True  \n",
       "3              True  \n",
       "4             False  \n",
       "5              True  \n",
       "6             False  \n",
       "7             False  \n",
       "8             False  \n",
       "9             False  \n",
       "\n",
       "[10 rows x 514 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76147087-381d-4971-8046-7b5cf7cdbb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'scores': [0.3613875508308411, 0.6386125087738037], 'classes': ['True', 'False']}\n",
      "{'classes': ['True', 'False'], 'scores': [0.4743289351463318, 0.5256710648536682]}\n",
      "{'scores': [0.9762881994247437, 0.02371174097061157], 'classes': ['True', 'False']}\n",
      "{'scores': [0.5234130620956421, 0.4765869975090027], 'classes': ['True', 'False']}\n",
      "{'classes': ['True', 'False'], 'scores': [0.03440927714109421, 0.9655907154083252]}\n",
      "{'classes': ['True', 'False'], 'scores': [0.0653722807765007, 0.9346277117729187]}\n",
      "{'scores': [0.04001647979021072, 0.9599835276603699], 'classes': ['True', 'False']}\n",
      "{'scores': [0.03490455448627472, 0.9650955200195312], 'classes': ['True', 'False']}\n",
      "{'classes': ['True', 'False'], 'scores': [0.4869522750377655, 0.5130476951599121]}\n",
      "{'scores': [0.0671602338552475, 0.9328398108482361], 'classes': ['True', 'False']}\n"
     ]
    }
   ],
   "source": [
    "for index, row in inference_sample.iterrows():\n",
    "    instance = json.loads(row.astype(str).to_json())\n",
    "    inference_results = predict_tabular_classification(PROJECT_NAME, LOCATION, 'projects/55590906972/locations/us-central1/endpoints/3602110043756953600', [instance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d739dee6-25cb-4fbc-9cc0-f06867675f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('sample_request.json', 'w') as outfile:\n",
    "    outfile.write(inference_sample.iloc[0].astype(str).to_json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad662fa-be4b-452e-80e8-9b51744b7524",
   "metadata": {},
   "source": [
    "### Undeploy Model from Vertex AI Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0e9c36-91ec-4da2-846d-05c9cbee68cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e79ea956-0440-4362-8602-864468604a57",
   "metadata": {},
   "source": [
    "### Pricing Notes\n",
    "Resources that incur costs\n",
    "Answer: you pay for three main activities\n",
    "- Training the model\n",
    "    - Price per node hour of tabular classification is $21.252, so that's the charge I incur every time I train the AutoML Beatles Model\n",
    "- Deploying the model to an endpoint (models must be deployed before they can make either online predictions or online evaluations)\n",
    "    - You pay for each model deployed to an endpoint, even if no prediction is made\n",
    "    - Must undeploy your model to stop incurring further charges\n",
    "    - Models that are not deployed or have failed to deploy are not charged\n",
    "- Using the model to make predictions; this is for both batch and online predictions (which I think is BS, since we're also paying to host the model at an endpoint, but whatever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8737fc2a-d702-4c7e-8903-60178cabc22c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m103"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
